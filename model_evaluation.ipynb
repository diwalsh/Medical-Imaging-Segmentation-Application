{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09ea206-e2f7-4bc6-8f50-9982911abcdb",
   "metadata": {},
   "source": [
    "# CNN Model Evaluation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b203b-9f44-4fd9-a83f-1151e8526aad",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "001c1515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:38.775081Z",
     "start_time": "2023-07-14T07:54:27.162224Z"
    },
    "executionInfo": {
     "elapsed": 6564,
     "status": "ok",
     "timestamp": 1713964814800,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "001c1515"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import platform\n",
    "import warnings\n",
    "import time\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# To filter UserWarning.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For data augmentation and preprocessing\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Imports required SegFormer classes\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# Importing lighting along with a built-in callback it provides.\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Importing torchmetrics modular and functional implementations.\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# To print model summary.\n",
    "#from torchinfo import summary\n",
    "\n",
    "# Tensor and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "880832c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:38.807356Z",
     "start_time": "2023-07-14T07:54:38.775081Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713964814801,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "880832c0"
   },
   "outputs": [],
   "source": [
    "# Sets the internal precision of float32 matrix multiplications.\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# To enable determinism.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "# To render the matplotlib figure in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fc3a642-e4df-4d08-b079-db9e134ca051",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = \"2d_all_v2\"\n",
    "MODEL_PATH = \"models\"\n",
    "CLASS_MODEL1 = \"resnet50v2_nn256_lr0001_relu_batch32_epoch30_v2.keras\"\n",
    "CLASS_MODEL2 = \"resnet50v2_nn256_lr0001_relu_batch64_epoch30_v4.keras\"\n",
    "SEG_MODEL = \"ckpt_epoch=049-vloss_val_loss=0.0000_vf1_valid_f1=0.0000.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843bec2-2fd5-49e8-a738-421ac54b6aee",
   "metadata": {},
   "source": [
    "# Configuration Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9191d6a-27dd-45f6-a04c-74181ab56315",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1713965426466,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "de70d1ed",
    "outputId": "17bab34d-e3f2-4cc2-8fca-71273e7671d3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "#@dataclass(frozen=True)\n",
    "#class DatasetClasConfig:\n",
    "#    IMAGE_SIZE: tuple[int,int] = (288, 288) # W, H\n",
    "#    MEAN: tuple = (0.136, 0.136, 0.136)\n",
    "#    STD:  tuple = (0.178, 0.178, 0.178)\n",
    "#    BATCH_SIZE = 32\n",
    "#    IMG_DIR = os.path.join(ROOT_PATH, \"valid\")\n",
    "#    MODEL_NAME = os.path.join(MODEL_PATH, CLASS_MODEL)\n",
    "#    MODEL_NPY = MODEL_NAME + '.npy'\n",
    "#    THR = 0.019938506186008453\n",
    "#    \n",
    "# Function for custom normalization\n",
    "#def custom_normalization(image):  \n",
    "#    image = image / 255.0    \n",
    "#    image = (image - DatasetClasConfig.MEAN[0]) / DatasetClasConfig.STD[0]\n",
    "#    return image\n",
    "    \n",
    "# Image preprocessing\n",
    "#test_datagen = ImageDataGenerator(\n",
    "#    preprocessing_function = custom_normalization)\n",
    "\n",
    "# Define the test set\n",
    "#test_generator = test_datagen.flow_from_directory(\n",
    "#    DatasetClasConfig.IMG_DIR,\n",
    "#    target_size = DatasetClasConfig.IMAGE_SIZE,\n",
    "#    batch_size = DatasetClasConfig.BATCH_SIZE,\n",
    "#    classes=['images'],\n",
    "#    shuffle=False\n",
    "#)\n",
    "\n",
    "# Load the classification model\n",
    "#class_model = load_model(DatasetClasConfig.MODEL_NAME)\n",
    "\n",
    "# Predict!\n",
    "#y_pred = class_model.predict(test_generator).reshape(-1)\n",
    "\n",
    "#MODEL_NPY = DatasetClasConfig.MODEL_NAME + '.npy'\n",
    "#np.save(DatasetClasConfig.MODEL_NPY, y_pred)\n",
    "#y_pred = np.load(DatasetClasConfig.MODEL_NPY)\n",
    "#THR = 0.1586085855960846\n",
    "#THR = 0.019938506186008453 #99.5% recall\n",
    "#CLASSIF_LABELS = y_pred > DatasetClasConfig.THR\n",
    "\n",
    "\n",
    "# Define configuration classes\n",
    "\n",
    "# A class that holds all the hyperparameters we will use to process images.\n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    NUM_CLASSES:   int = 4 # including background.\n",
    "    IMAGE_SIZE: tuple[int,int] = (288, 288) # W, H\n",
    "    MEAN: tuple = (0.485, 0.456, 0.406)\n",
    "    STD:  tuple = (0.229, 0.224, 0.225)\n",
    "    MEAN_CLF: float = 0.136\n",
    "    STD_CLF: float = 0.178\n",
    "    BACKGROUND_CLS_ID: int = 0\n",
    "    URL: str = r\"https://www.dropbox.com/scl/fi/r0685arupp33sy31qhros/dataset_UWM_GI_Tract_train_valid.zip?rlkey=w4ga9ysfiuz8vqbbywk0rdnjw&dl=1\"\n",
    "    DATASET_PATH: str = os.path.join(os.getcwd(), ROOT_PATH)\n",
    "    MODEL_NAME_CLF1 = os.path.join(MODEL_PATH, CLASS_MODEL1)\n",
    "    MODEL_NAME_CLF2 = os.path.join(MODEL_PATH, CLASS_MODEL2)\n",
    "    MODEL_NAME_SEG = os.path.join(MODEL_PATH, SEG_MODEL)\n",
    "    THR1: float = 0.019938506186008453\n",
    "    THR2: float = 0.07652711868286133\n",
    "\n",
    "# A class containing the locations of the images and masks of these model evaluation tests\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    DATA_VALID_IMAGES: str = os.path.join(DatasetConfig.DATASET_PATH, \"valid\", \"images\", r\"*.png\")\n",
    "    DATA_VALID_LABELS: str = os.path.join(DatasetConfig.DATASET_PATH, \"valid\", \"masks\",  r\"*.png\")\n",
    "\n",
    "# A class that holds all the hyperparameters for training and evaluation.\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:      int = 12 # 8\n",
    "    NUM_EPOCHS:      int = 1\n",
    "    INIT_LR:       float = 3e-4\n",
    "    NUM_WORKERS:     int = 0 if platform.system() == \"Windows\" else os.cpu_count()\n",
    "\n",
    "    OPTIMIZER_NAME:  str = \"AdamW\"\n",
    "    WEIGHT_DECAY:  float = 1e-4\n",
    "    USE_SCHEDULER:  bool = True # Use learning rate scheduler?\n",
    "    SCHEDULER:       str = \"MultiStepLR\" # Name of the scheduler to use.\n",
    "    MODEL_NAME:str = \"nvidia/segformer-b4-finetuned-ade-512-512\"\n",
    "\n",
    "# A class that contains the (optional) batch size and the number of batches to be used to display for inference results.\n",
    "#@dataclass\n",
    "class InferenceConfig:\n",
    "    BATCH_SIZE:  int = 10\n",
    "    NUM_BATCHES: int = 2\n",
    "\n",
    "DatasetConfig.NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd876f-0761-4b09-a009-001034e524d1",
   "metadata": {},
   "source": [
    "# Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11e76e6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.876Z"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1713965429746,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "11e76e6b"
   },
   "outputs": [],
   "source": [
    "# Custom Class MedicalDataset for creating training and validation (segmentation) dataset objects.\n",
    "\n",
    "# Class inherited from Pytorch Dataset\n",
    "class MedicalDataset(Dataset):\n",
    "    # Initialization function\n",
    "    def __init__(self, *, image_paths, mask_paths, img_size, ds_mean, ds_std, ds_mean_clf, ds_std_clf):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean = ds_mean\n",
    "        self.ds_std = ds_std\n",
    "        self.ds_mean_clf = ds_mean_clf\n",
    "        self.ds_std_clf = ds_std_clf\n",
    "        self.transforms  = self.setup_transforms(mean=self.ds_mean, std=self.ds_std)\n",
    "\n",
    "    # Calculates the number of images used for training or validation\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # Normalization Function for binary classification (stomach/small/large bowel: yes, no)\n",
    "    def normalize_classif(self, image):\n",
    "        image = image / 255.0\n",
    "        image = (image - self.ds_mean_clf) / self.ds_std_clf\n",
    "        return image\n",
    "\n",
    "    # Preprocess transforms - Normalization and converting to PyTorch tensor format (HWC --> CHW).\n",
    "    def setup_transforms(self, *, mean, std):\n",
    "        transforms = []        \n",
    "        transforms.extend([\n",
    "                A.Normalize(mean=mean, std=std, always_apply=True),\n",
    "                ToTensorV2(always_apply=True),  # (H, W, C) --> (C, H, W)\n",
    "        ])\n",
    "        return A.Compose(transforms)\n",
    "\n",
    "    # load and resize the image to the specified size.\n",
    "    # The interpolation method used is nearest-neighbor for the segmentation model.\n",
    "    def load_file_nearest(self, file_path, depth=0):\n",
    "        file = cv2.imread(file_path, depth)\n",
    "        if depth == cv2.IMREAD_COLOR:\n",
    "            file = file[:, :, ::-1]\n",
    "        return cv2.resize(file, (self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # load and resize the image to the specified size.\n",
    "    # The interpolation method used is linear for the classification model.\n",
    "    def load_file_linear(self, file_path, depth=0):\n",
    "        file = cv2.imread(file_path, depth)\n",
    "        if depth == cv2.IMREAD_COLOR:\n",
    "            file = file[:, :, ::-1]\n",
    "        return cv2.resize(file, (self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load and preprocess image for classification\n",
    "        image_clf = self.load_file_linear(self.image_paths[index], depth=cv2.IMREAD_COLOR)\n",
    "        image_clf = self.normalize_classif(image_clf)\n",
    "        \n",
    "        # Load and preprocess image and mask file, both for segmentation\n",
    "        image = self.load_file_nearest(self.image_paths[index], depth=cv2.IMREAD_COLOR)       \n",
    "        mask  = self.load_file_nearest(self.mask_paths[index],  depth=cv2.IMREAD_GRAYSCALE)        \n",
    "        transformed = self.transforms(image=image, mask=mask)\n",
    "\n",
    "        # Return the loeaded images\n",
    "        image, mask = transformed[\"image\"], transformed[\"mask\"].to(torch.long)        \n",
    "        return image_clf, image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dc1d17b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.880Z"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1713965435112,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "3dc1d17b"
   },
   "outputs": [],
   "source": [
    "# Class MedicalSegmentationDataModule helps organize and encapsulate all the data-related operations and logic\n",
    "# in a PyTorch project. It acts as a bridge between your data and Lightning’s training pipeline. It is a convenient\n",
    "# abstraction that encapsulates data-related operations, promotes code organization, and facilitates seamless integration\n",
    "# with other Lightning components for efficient and reproducible deep-learning experiments.\n",
    "\n",
    "#LightningDataModule: a datamodule that encapsulates the five steps involved in data processing in PyTorch: Download / tokenize / process; \n",
    "# Clean and (maybe) save to disk; Load inside Dataset; Apply transforms (rotate, tokenize, etc…); Wrap inside a DataLoader.\n",
    "\n",
    "class MedicalSegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_size=(384, 384),\n",
    "        ds_mean=(0.485, 0.456, 0.406),\n",
    "        ds_std=(0.229, 0.224, 0.225),\n",
    "        ds_mean_clf=0.136,\n",
    "        ds_std_clf=0.178,\n",
    "        batch_size=12,\n",
    "        num_workers=3,\n",
    "        pin_memory=False,\n",
    "        shuffle_validation=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean     = ds_mean\n",
    "        self.ds_std      = ds_std\n",
    "        self.ds_mean_clf = ds_mean_clf\n",
    "        self.ds_std_clf  = ds_std_clf\n",
    "        self.batch_size  = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory  = pin_memory\n",
    "        self.shuffle_validation = shuffle_validation\n",
    "\n",
    "    # Create validation dataset and dataloader.\n",
    "    def setup(self, *args, **kwargs):        \n",
    "        valid_imgs = sorted(glob(f\"{Paths.DATA_VALID_IMAGES}\"))        \n",
    "        valid_msks = sorted(glob(f\"{Paths.DATA_VALID_LABELS}\"))\n",
    "        self.valid_ds = MedicalDataset(image_paths=valid_imgs, mask_paths=valid_msks, img_size=self.img_size, \n",
    "                                       ds_mean=self.ds_mean, ds_std=self.ds_std,\n",
    "                                       ds_mean_clf=self.ds_mean_clf, ds_std_clf=self.ds_std_clf)\n",
    "\n",
    "    # Create validation dataloader object.\n",
    "    def val_dataloader(self):        \n",
    "        return DataLoader(\n",
    "            self.valid_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory,\n",
    "            num_workers=self.num_workers, shuffle=self.shuffle_validation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec06e142",
   "metadata": {
    "editable": true,
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1713965490343,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "ec06e142",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading a pre-trained model version\n",
    "def get_model(*, model_name, num_classes):\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "class MedicalSegmentationModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_classes: int = 10,\n",
    "        init_lr: float = 0.001,\n",
    "        optimizer_name: str = \"Adam\",\n",
    "        weight_decay: float = 1e-4,\n",
    "        use_scheduler: bool = False,\n",
    "        scheduler_name: str = \"multistep_lr\",\n",
    "        num_epochs: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save the arguments as hyperparameters.\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Loading model using the function defined above.\n",
    "        self.model = get_model(model_name=self.hparams.model_name, num_classes=self.hparams.num_classes)\n",
    "\n",
    "        # Initializing the required metric objects.\n",
    "        self.mean_train_loss = MeanMetric()\n",
    "        self.mean_train_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "        self.mean_valid_loss = MeanMetric()\n",
    "        self.mean_valid_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        outputs = self.model(pixel_values=data, return_dict=True)\n",
    "        upsampled_logits = F.interpolate(outputs[\"logits\"], size=data.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return upsampled_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d94d5-9ee8-42fa-9f64-ecb276ab74c0",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "60294fe7-c252-4c13-81bf-4ed210306cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dice coefficient\n",
    "def dice_coef(predictions, ground_truths, num_classes=4, dims=(1, 2), smooth=1e-8):\n",
    "    #print(predictions.shape)\n",
    "    #print(type(predictions))\n",
    "    #print(ground_truths.shape)\n",
    "    #print(type(ground_truths))\n",
    "    # (batch_size, height, width, num_classes)\n",
    "    ground_truth_oh = F.one_hot(ground_truths, num_classes=num_classes)\n",
    "    # (batch_size, height, width, num_classes)\n",
    "    prediction_norm = F.one_hot(predictions, num_classes=num_classes)\n",
    "\n",
    "    #print(prediction_norm.shape)\n",
    "    #print(ground_truth_oh.shape)\n",
    "    \n",
    "    # (batch_size, num_classes)\n",
    "    intersection = (prediction_norm * ground_truth_oh).sum(dim=dims)\n",
    "    # (batch_size, num_classes)\n",
    "    summation = prediction_norm.sum(dim=dims) + ground_truth_oh.sum(dim=dims)\n",
    "    # (batch_size, num_classes)\n",
    "    dice = (2.0 * intersection + smooth) / (summation + smooth)\n",
    "    dice_mean = dice.mean()\n",
    "\n",
    "    return dice_mean\n",
    "\n",
    "# Detect if the mask includes any of the classes\n",
    "def detect_nonzero_masks(masks):\n",
    "    # Check if any element is nonzero along the last two axes\n",
    "    nonzero_masks_mask = np.any(masks != 0, axis=(1, 2))\n",
    "    # Get the indices of the zero images\n",
    "    nonzero_masks_indices = np.where(nonzero_masks_mask)[0]\n",
    "    return nonzero_masks_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3e243e55-73e3-4ad3-a3a9-0f669fa8f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that performs the evaluation process of the model\n",
    "@torch.inference_mode()\n",
    "def inference(class_model, seg_model, loader, img_size, device=\"cpu\", enable_clf=False, threshold_clf=0):\n",
    "    \n",
    "    cont = 0\n",
    "    cont_2 = 0\n",
    "    score_sum = 0\n",
    "    score_sum_2 = 0\n",
    "    score = 1.0\n",
    "    score_2 = 1.0\n",
    "    score_ave = 1.0\n",
    "    score_ave_2 = 1.0\n",
    "    time_acc = 0\n",
    "\n",
    "    for idx, (batch_img_clf, batch_img_seg, batch_mask) in enumerate(loader):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        if enable_clf: # Classification -> Segmentation\n",
    "\n",
    "            pred_all = torch.Tensor((np.zeros(batch_mask.shape))).long()\n",
    "            \n",
    "            # Classification predictions\n",
    "            y_pred_clf = class_model.predict(batch_img_clf, verbose=0).reshape(-1)\n",
    "            clf_labels = y_pred_clf > threshold_clf\n",
    "            true_idxs = np.where(clf_labels == True)[0]\n",
    "\n",
    "            # Segmentation predictions\n",
    "            if len(true_idxs) > 0:\n",
    "                predictions = seg_model(batch_img_seg[true_idxs].to(device))\n",
    "                pred_all[true_idxs] = torch.from_numpy(predictions.argmax(dim=1).cpu().numpy())\n",
    "                pred_all = torch.tensor(pred_all)\n",
    "                \n",
    "            \n",
    "            #predictions = seg_model(batch_img_seg.to(device))\n",
    "            #pred_all = predictions.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            # Re-label to zero the masks that have been identified as Class 0 by the classifier\n",
    "            #pred_all[np.where(clf_labels == False)] = 0\n",
    "            #pred_all = torch.tensor(pred_all)\n",
    "            \n",
    "        else:\n",
    "            # Segmentation predictions\n",
    "            predictions = seg_model(batch_img_seg.to(device))\n",
    "            pred_all = predictions.argmax(dim=1).cpu().numpy()\n",
    "            pred_all = torch.tensor(pred_all)\n",
    "\n",
    "        # Compute modeling time\n",
    "        end = time.perf_counter()\n",
    "        time_acc = time_acc + (end - start)\n",
    "\n",
    "        # Compute Dice coefficient\n",
    "        cont = cont + 1\n",
    "        score = dice_coef(pred_all, batch_mask).numpy()        \n",
    "        score_sum = score_sum + score\n",
    "        score_ave = score_sum / cont\n",
    "        \n",
    "        nonzero_mask_idxs = detect_nonzero_masks(batch_mask.numpy())\n",
    "        \n",
    "        if len(nonzero_mask_idxs) > 0:\n",
    "            cont_2 = cont_2 + 1\n",
    "            score_2 = dice_coef(pred_all[nonzero_mask_idxs],batch_mask[nonzero_mask_idxs]).numpy()\n",
    "            score_sum_2 = score_sum_2 + score_2\n",
    "            score_ave_2 = score_sum_2 / cont_2      \n",
    "\n",
    "        if idx == InferenceConfig.NUM_BATCHES * 5:\n",
    "            break\n",
    "                          \n",
    "    time_per_image = time_acc / cont\n",
    "\n",
    "    print(\"-----\" * 7)\n",
    "    print(f\"ave dice_score (all masks): {np.round(score_ave,3)}\")\n",
    "    print(f\"ave dice_score (only segmented): {np.round(score_ave_2,3)}\") \n",
    "    print(f\"Segmentation time per image: {np.round(time_per_image,3)}\")\n",
    "\n",
    "    return score_ave, score_ave_2, time_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03ae43-fca6-47c4-8a12-6f2974cd9fca",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24dacc05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1713965496076,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "24dacc05",
    "outputId": "9c6eee87-3eb5-405a-d720-668ea35448e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Seed everything for reproducibility.\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Get the validation dataloader.\n",
    "data_module = MedicalSegmentationDataModule(\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    img_size=DatasetConfig.IMAGE_SIZE,\n",
    "    ds_mean=DatasetConfig.MEAN,\n",
    "    ds_std=DatasetConfig.STD,\n",
    "    ds_mean_clf=DatasetConfig.MEAN_CLF,\n",
    "    ds_std_clf=DatasetConfig.STD_CLF,\n",
    "    batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    num_workers=TrainingConfig.NUM_WORKERS,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "data_module.setup()\n",
    "valid_loader = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef5fb7b6-33ad-4a3f-876b-4c01aa0a57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification model\n",
    "class_model1 = load_model(DatasetConfig.MODEL_NAME_CLF1)\n",
    "#class_model2 = load_model(DatasetConfig.MODEL_NAME_CLF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a5a5d1e-f228-4544-a9a6-0aa2d0b4ec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([4, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load segmentation model (SegFormer 1)\n",
    "seg_model = MedicalSegmentationModel.load_from_checkpoint(DatasetConfig.MODEL_NAME_SEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0a5b0-2312-4b97-bc17-e89530e4e631",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4472f-098a-40f1-87f1-56f52e8cb32f",
   "metadata": {},
   "source": [
    "## SegFormer 1 (Trained With Only Segmented Slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac4d2856-7052-4379-b4c9-9c7f30d50076",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3: <class 'numpy.ndarray'>\n",
      " 3: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m seg_model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#seg_model.eval()\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m score_ave, score_ave_2, time_per_image \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_model1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDatasetConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_clf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mave dice_score (all masks): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mround(score_ave,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[89], line 51\u001b[0m, in \u001b[0;36minference\u001b[1;34m(class_model, seg_model, loader, img_size, device, enable_clf, threshold_clf)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pred_all)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m#    predictions = seg_model(batch_img_seg[true_idxs,:,:].to(device))\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#print(predictions.shape)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Segmentation predictions\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mseg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_img_seg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     pred_all \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 3: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pred_all)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 37\u001b[0m, in \u001b[0;36mMedicalSegmentationModel.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m---> 37\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     upsampled_logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m], size\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m upsampled_logits\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:791\u001b[0m, in \u001b[0;36mSegformerForSemanticSegmentation.forward\u001b[1;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    786\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    787\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    788\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    789\u001b[0m )\n\u001b[1;32m--> 791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we need the intermediate hidden states\u001b[39;49;00m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    800\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_head(encoder_hidden_states)\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:548\u001b[0m, in \u001b[0;36mSegformerModel.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    543\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    544\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    545\u001b[0m )\n\u001b[0;32m    546\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 548\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:422\u001b[0m, in \u001b[0;36mSegformerEncoder.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    420\u001b[0m embedding_layer, block_layer, norm_layer \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# first, obtain patch embeddings\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m hidden_states, height, width \u001b[38;5;241m=\u001b[39m \u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# second, send embeddings through blocks\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(block_layer):\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:137\u001b[0m, in \u001b[0;36mSegformerOverlapPatchEmbeddings.forward\u001b[1;34m(self, pixel_values)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[1;32m--> 137\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     _, _, height, width \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# (batch_size, num_channels, height, width) -> (batch_size, num_channels, height*width) -> (batch_size, height*width, num_channels)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# this can be fed to a Transformer layer\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model.to(DEVICE)\n",
    "#seg_model.eval()\n",
    "\n",
    "score_ave, score_ave_2, time_per_image = inference(class_model1, seg_model, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=False)\n",
    "\n",
    "print(\"-----\" * 7)\n",
    "print(f\"ave dice_score (all masks): {np.round(score_ave,3)}\")\n",
    "print(f\"ave dice_score (only segmented): {np.round(score_ave_2,3)}\") \n",
    "print(f\"Segmentation time per image: {np.round(time_per_image,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43337381-dcbc-45fe-99ea-9bf0b063257d",
   "metadata": {},
   "source": [
    "## Classifier 1 + SegFormer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a0ea86d2-effc-43cb-88aa-dc4c69d10304",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "ave dice_score (all masks): 0.948\n",
      "ave dice_score (only segmented): 0.951\n",
      "Segmentation time per image: 13.703\n",
      "-----------------------------------\n",
      "ave dice_score (all masks): 0.948\n",
      "ave dice_score (only segmented): 0.951\n",
      "Segmentation time per image: 13.703\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model.to(DEVICE)\n",
    "#seg_model.eval()\n",
    "\n",
    "score_ave, score_ave_2, time_per_image = inference(class_model1, seg_model, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=True, threshold_clf=DatasetConfig.THR1)\n",
    "\n",
    "print(\"-----\" * 7)\n",
    "print(f\"ave dice_score (all masks): {np.round(score_ave,3)}\")\n",
    "print(f\"ave dice_score (only segmented): {np.round(score_ave_2,3)}\") \n",
    "print(f\"Segmentation time per image: {np.round(time_per_image,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487479fa-a24f-4b76-a431-3e871971e454",
   "metadata": {},
   "source": [
    "## Classifier 2 + SegFormer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f8daa64-5f9b-41e9-896c-6daba8588a78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07652711868286133\n",
      "-----------------------------------\n",
      "ave dice_score (all masks): 0.941\n",
      "ave dice_score (only segmented): 0.951\n",
      "Segmentation time per image: 3.239\n"
     ]
    }
   ],
   "source": [
    "# Load classification model\n",
    "#class_model = load_model(DatasetConfig.MODEL_NAME_CLF2)\n",
    "\n",
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model.to(DEVICE)\n",
    "#seg_model.eval()\n",
    "score_ave, score_ave_2, time_per_image = inference(class_model2, seg_model, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=True, threshold_clf=DatasetConfig.THR2)\n",
    "\n",
    "print(\"-----\" * 7)\n",
    "print(f\"ave dice_score (all masks): {np.round(score_ave,3)}\")\n",
    "print(f\"ave dice_score (only segmented): {np.round(score_ave_2,3)}\") \n",
    "print(f\"Segmentation time per image: {np.round(time_per_image,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a1a42-a62a-4b25-ad49-93883fafee45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_common",
   "language": "python",
   "name": ".venv_common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299.896px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1378ba22f96e4134a3aadf23f395c097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50cd2c575f6345f5a159bfc600484d0e",
       "IPY_MODEL_86f3315ff10144f095aea40c461d256e",
       "IPY_MODEL_1700c995be8b45e2ae7676ae73dc88c5"
      ],
      "layout": "IPY_MODEL_3552cf8af21745129c9750e6ca619f29"
     }
    },
    "1700c995be8b45e2ae7676ae73dc88c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84878029a4fe45798f0c4728b738652d",
      "placeholder": "​",
      "style": "IPY_MODEL_4467c1d174224764bf4c0d4a8abe8dca",
      "value": " 2/2 [00:00&lt;00:00,  4.49it/s]"
     }
    },
    "17038ea0beb5498c8ef4b792cc4f78f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "178ab317e5ae44ad8640feb4c4db4837": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "23d8be2f4b884a0c9544b546a292f9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_714473c7d20848b6a742abfea94f3c52",
      "max": 280,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec5bbe8822bd4b34a784726a7671bca6",
      "value": 280
     }
    },
    "24fdc15d41624e5682b2e8322ea3a3f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a54a04435044109a737ba66ade2aafc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3552cf8af21745129c9750e6ca619f29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "4467c1d174224764bf4c0d4a8abe8dca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49140305690348abb2a551d3bdd3849e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24fdc15d41624e5682b2e8322ea3a3f0",
      "placeholder": "​",
      "style": "IPY_MODEL_738e6f0aea304c50a039395809b48216",
      "value": "Epoch 0: 100%"
     }
    },
    "50cd2c575f6345f5a159bfc600484d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a54a04435044109a737ba66ade2aafc",
      "placeholder": "​",
      "style": "IPY_MODEL_c195462a99934f639c166547547f433c",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "6647710a025145c09dc750f2dbf889bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f3f92ccc6324c1bbc1e4458b54ba521",
       "IPY_MODEL_23d8be2f4b884a0c9544b546a292f9bc",
       "IPY_MODEL_85d41b54e32d48a789c88379eb74d329"
      ],
      "layout": "IPY_MODEL_6c1a05d1bcb94ab5993b861b1a9b9c00"
     }
    },
    "6c1a05d1bcb94ab5993b861b1a9b9c00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "714473c7d20848b6a742abfea94f3c52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "738e6f0aea304c50a039395809b48216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76c71109197449199fe6b5126cbfdfcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8297e891e764434cb1c8007e6adb9fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84878029a4fe45798f0c4728b738652d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85d41b54e32d48a789c88379eb74d329": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe66361110e5430fb8e721025f6c7e75",
      "placeholder": "​",
      "style": "IPY_MODEL_76c71109197449199fe6b5126cbfdfcd",
      "value": " 280/280 [01:37&lt;00:00,  2.87it/s]"
     }
    },
    "86f3315ff10144f095aea40c461d256e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e654fa4cd4314f7a9f070558a7119659",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9a2e9b44f294265a9c7cacf4678d999",
      "value": 2
     }
    },
    "9f3f92ccc6324c1bbc1e4458b54ba521": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c306eacaeb3b47019480caf4a0467a03",
      "placeholder": "​",
      "style": "IPY_MODEL_d134b87624894ab4946ec1c07f0fa48d",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "be22a7e68186419f8bb2b1f6ff525402": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c195462a99934f639c166547547f433c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c306eacaeb3b47019480caf4a0467a03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9a2e9b44f294265a9c7cacf4678d999": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d134b87624894ab4946ec1c07f0fa48d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1d57cb119c644148b5ccaeb79dd60cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17038ea0beb5498c8ef4b792cc4f78f0",
      "placeholder": "​",
      "style": "IPY_MODEL_f3d9346390a4463c8d916d58101e290d",
      "value": " 1103/1103 [19:22&lt;00:00,  0.95it/s, v_num=zws9, train/batch_loss=nan.0, train/batch_f1=nan.0, valid/loss=nan.0, valid/f1=0.245]"
     }
    },
    "e59fc8cc7f784fceaf205d97685d263d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49140305690348abb2a551d3bdd3849e",
       "IPY_MODEL_f05896a64ca640d085018265a319cec3",
       "IPY_MODEL_e1d57cb119c644148b5ccaeb79dd60cf"
      ],
      "layout": "IPY_MODEL_178ab317e5ae44ad8640feb4c4db4837"
     }
    },
    "e654fa4cd4314f7a9f070558a7119659": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec5bbe8822bd4b34a784726a7671bca6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f05896a64ca640d085018265a319cec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be22a7e68186419f8bb2b1f6ff525402",
      "max": 1103,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8297e891e764434cb1c8007e6adb9fa4",
      "value": 1103
     }
    },
    "f3d9346390a4463c8d916d58101e290d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe66361110e5430fb8e721025f6c7e75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
