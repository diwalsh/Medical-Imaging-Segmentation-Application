{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2208608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:27.162224Z",
     "start_time": "2023-07-14T07:54:27.146552Z"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1713964806317,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "d2208608"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import platform\n",
    "import warnings\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# To filter UserWarning.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001c1515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:38.775081Z",
     "start_time": "2023-07-14T07:54:27.162224Z"
    },
    "executionInfo": {
     "elapsed": 6564,
     "status": "ok",
     "timestamp": 1713964814800,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "001c1515"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssre_\\.venv_common\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# For data augmentation and preprocessing.\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Imports required SegFormer classes\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# Importing lighting along with a built-in callback it provides.\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Importing torchmetrics modular and functional implementations.\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "# To print model summary.\n",
    "from torchinfo import summary\n",
    "\n",
    "# Tensor and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880832c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:38.807356Z",
     "start_time": "2023-07-14T07:54:38.775081Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713964814801,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "880832c0"
   },
   "outputs": [],
   "source": [
    "# Sets the internal precision of float32 matrix multiplications.\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# To enable determinism.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "# To render the matplotlib figure in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc3a642-e4df-4d08-b079-db9e134ca051",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"C:/Users/ssre_/Projects/dats-data/2d_all_v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843bec2-2fd5-49e8-a738-421ac54b6aee",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a76e57fc-2853-4e17-8a07-b7337f8767a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6560 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True)\n",
    "\n",
    "# Define constants\n",
    "class DatasetClasConfig:\n",
    "    IMAGE_SIZE: tuple[int,int] = (288, 288) # W, H\n",
    "    MEAN: tuple = (0.136, 0.136, 0.136)\n",
    "    STD:  tuple = (0.178, 0.178, 0.178)\n",
    "    BATCH_SIZE = 32\n",
    "    IMG_DIR = os.path.join(ROOT_DIR, \"valid\")\n",
    "    MODEL_DIR = \"C:/Users/ssre_/Projects/dats-data/models\"\n",
    "    \n",
    "# Function for custom normalization\n",
    "def custom_normalization(image):  \n",
    "    image = image / 255.0    \n",
    "    image = (image - DatasetClasConfig.MEAN[0]) / DatasetClasConfig.STD[0]\n",
    "    return image\n",
    "    \n",
    "# Image preprocessing\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = custom_normalization)\n",
    "\n",
    "# Define the test set\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    DatasetClasConfig.IMG_DIR,\n",
    "    target_size = DatasetClasConfig.IMAGE_SIZE,\n",
    "    batch_size = DatasetClasConfig.BATCH_SIZE,\n",
    "    classes=['images'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the classification model\n",
    "#class_model_name = os.path.join(DatasetClasConfig.MODEL_DIR, \"resnet50v2_nn256_lr0001_relu_batch32_epoch30_v2_2p5d.keras\")\n",
    "#class_model = load_model(class_model_name)\n",
    "\n",
    "# Predict!\n",
    "#y_pred = class_model.predict(test_generator).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e52a9a9-0d16-4ce1-b1b2-d5ea87b03a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "#np.save('classification_labels.npy', y_pred)\n",
    "prediction = np.load('classification_labels.npy')\n",
    "THR = 0.1586085855960846\n",
    "CLASSIF_LABELS = prediction > THR\n",
    "print(CLASSIF_LABELS[0:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e087ad-3e46-4e34-a538-19903ba6d53c",
   "metadata": {},
   "source": [
    "# Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1683f5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.866Z"
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1713965421773,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "ae1683f5"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    NUM_CLASSES:   int = 4 # including background.\n",
    "    IMAGE_SIZE: tuple[int,int] = (288, 288) # W, H\n",
    "    MEAN: tuple = (0.485, 0.456, 0.406)\n",
    "    STD:  tuple = (0.229, 0.224, 0.225)\n",
    "    BACKGROUND_CLS_ID: int = 0\n",
    "    URL: str = r\"https://www.dropbox.com/scl/fi/r0685arupp33sy31qhros/dataset_UWM_GI_Tract_train_valid.zip?rlkey=w4ga9ysfiuz8vqbbywk0rdnjw&dl=1\"\n",
    "    DATASET_PATH: str = os.path.join(os.getcwd(), ROOT_DIR)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    DATA_TRAIN_IMAGES: str = os.path.join(DatasetConfig.DATASET_PATH, \"train\", \"images\", r\"*.png\")\n",
    "    DATA_TRAIN_LABELS: str = os.path.join(DatasetConfig.DATASET_PATH, \"train\", \"masks\",  r\"*.png\")\n",
    "    DATA_VALID_IMAGES: str = os.path.join(DatasetConfig.DATASET_PATH, \"valid\", \"images\", r\"*.png\")\n",
    "    DATA_VALID_LABELS: str = os.path.join(DatasetConfig.DATASET_PATH, \"valid\", \"masks\",  r\"*.png\")\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:      int = 12 # 8\n",
    "    NUM_EPOCHS:      int = 1\n",
    "    INIT_LR:       float = 3e-4\n",
    "    NUM_WORKERS:     int = 0 if platform.system() == \"Windows\" else os.cpu_count()\n",
    "\n",
    "    OPTIMIZER_NAME:  str = \"AdamW\"\n",
    "    WEIGHT_DECAY:  float = 1e-4\n",
    "    USE_SCHEDULER:  bool = True # Use learning rate scheduler?\n",
    "    SCHEDULER:       str = \"MultiStepLR\" # Name of the scheduler to use.\n",
    "    MODEL_NAME:str = \"nvidia/segformer-b4-finetuned-ade-512-512\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    BATCH_SIZE:  int = 10\n",
    "    NUM_BATCHES: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de70d1ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1713965426466,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "de70d1ed",
    "outputId": "17bab34d-e3f2-4cc2-8fca-71273e7671d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes 4\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping of class ID to RGB value.\n",
    "id2color = {\n",
    "    0: (0, 0, 0),    # background pixel\n",
    "    1: (0, 0, 255),  # Stomach\n",
    "    2: (0, 255, 0),  # Small Bowel\n",
    "    3: (255, 0, 0),  # large Bowel\n",
    "}\n",
    "\n",
    "DatasetConfig.NUM_CLASSES = len(id2color)\n",
    "\n",
    "print(\"Number of classes\", DatasetConfig.NUM_CLASSES)\n",
    "\n",
    "# Reverse id2color mapping.\n",
    "# Used for converting RGB mask to a single channel (grayscale) representation.\n",
    "rev_id2color = {value: key for key, value in id2color.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e76e6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.876Z"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1713965429746,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "11e76e6b"
   },
   "outputs": [],
   "source": [
    "# Custom Class for creating training and validation (segmentation) dataset objects.\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    #def __init__(self, *, image_paths, mask_paths, img_size, ds_mean, ds_std, is_train=False):\n",
    "    def __init__(self, *, image_paths, mask_paths, classif_labels, img_size, ds_mean, ds_std, is_train=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "        self.classif_labels = classif_labels\n",
    "        self.is_train    = is_train\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean = ds_mean\n",
    "        self.ds_std = ds_std\n",
    "        self.transforms  = self.setup_transforms(mean=self.ds_mean, std=self.ds_std)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def setup_transforms(self, *, mean, std):\n",
    "        transforms = []\n",
    "\n",
    "        # Augmentation to be applied to the training set.\n",
    "        if self.is_train:\n",
    "            transforms.extend([\n",
    "                A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(scale_limit=0.12, rotate_limit=0.15, shift_limit=0.12, p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.CoarseDropout(max_holes=8, max_height=self.img_size[1]//20, max_width=self.img_size[0]//20, min_holes=5, fill_value=0, mask_fill_value=0, p=0.5)\n",
    "            ])\n",
    "\n",
    "        # Preprocess transforms - Normalization and converting to PyTorch tensor format (HWC --> CHW).\n",
    "        transforms.extend([\n",
    "                A.Normalize(mean=mean, std=std, always_apply=True),\n",
    "                ToTensorV2(always_apply=True),  # (H, W, C) --> (C, H, W)\n",
    "        ])\n",
    "        return A.Compose(transforms)\n",
    "\n",
    "    def load_file(self, file_path, depth=0):\n",
    "        file = cv2.imread(file_path, depth)\n",
    "        if depth == cv2.IMREAD_COLOR:\n",
    "            file = file[:, :, ::-1]\n",
    "        return cv2.resize(file, (self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image and mask file.\n",
    "        image = self.load_file(self.image_paths[index], depth=cv2.IMREAD_COLOR)\n",
    "        mask  = self.load_file(self.mask_paths[index],  depth=cv2.IMREAD_GRAYSCALE)\n",
    "        labels = self.classif_labels[index]\n",
    "        \n",
    "        # Apply Preprocessing (+ Augmentations) transformations to image-mask pair\n",
    "        transformed = self.transforms(image=image, mask=mask)\n",
    "        image, mask = transformed[\"image\"], transformed[\"mask\"].to(torch.long)\n",
    "        return image, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc1d17b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.880Z"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1713965435112,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "3dc1d17b"
   },
   "outputs": [],
   "source": [
    "class MedicalSegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_size=(384, 384),\n",
    "        ds_mean=(0.485, 0.456, 0.406),\n",
    "        ds_std=(0.229, 0.224, 0.225),\n",
    "        batch_size=12,\n",
    "        num_workers=3,\n",
    "        pin_memory=False,\n",
    "        shuffle_validation=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean     = ds_mean\n",
    "        self.ds_std      = ds_std\n",
    "        self.batch_size  = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory  = pin_memory\n",
    "\n",
    "        self.shuffle_validation = shuffle_validation\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download dataset.\n",
    "        dataset_zip_path = f\"{DatasetConfig.DATASET_PATH}.zip\"\n",
    "\n",
    "        # Download if dataset does not exists.\n",
    "        if not os.path.exists(DatasetConfig.DATASET_PATH):\n",
    "\n",
    "            print(\"Downloading and extracting assets...\", end=\"\")\n",
    "            file = requests.get(DatasetConfig.URL)\n",
    "            open(dataset_zip_path, \"wb\").write(file.content)\n",
    "\n",
    "            try:\n",
    "                with zipfile.ZipFile(dataset_zip_path) as z:\n",
    "                    z.extractall(os.path.split(dataset_zip_path)[0]) # Unzip where downloaded.\n",
    "                    print(\"Done\")\n",
    "            except:\n",
    "                print(\"Invalid file\")\n",
    "\n",
    "            os.remove(dataset_zip_path) # Remove the ZIP file to free storage space.\n",
    "\n",
    "    def setup(self, classif_labels=np.array([]), *args, **kwargs):\n",
    "        # Create training dataset and dataloader.\n",
    "        train_imgs = sorted(glob(f\"{Paths.DATA_TRAIN_IMAGES}\"))\n",
    "        train_msks  = sorted(glob(f\"{Paths.DATA_TRAIN_LABELS}\"))\n",
    "\n",
    "        # Create validation dataset and dataloader.\n",
    "        valid_imgs = sorted(glob(f\"{Paths.DATA_VALID_IMAGES}\"))\n",
    "        valid_imgs_2 = (glob(f\"{Paths.DATA_VALID_IMAGES}\"))\n",
    "        valid_msks = sorted(glob(f\"{Paths.DATA_VALID_LABELS}\"))\n",
    "        if valid_imgs == valid_imgs_2:\n",
    "            print(\"OK!\")\n",
    "        if len(classif_labels) == 0:\n",
    "            classif_labels = np.ones(len(valid_imgs))\n",
    "        else:\n",
    "            print(classif_labels[0:12])\n",
    "       \n",
    "        self.train_ds = MedicalDataset(image_paths=train_imgs, mask_paths=train_msks, classif_labels=classif_labels,\n",
    "                                       img_size=self.img_size, is_train=True, ds_mean=self.ds_mean, ds_std=self.ds_std)\n",
    "\n",
    "        self.valid_ds = MedicalDataset(image_paths=valid_imgs, mask_paths=valid_msks, classif_labels=classif_labels,\n",
    "                                       img_size=self.img_size, is_train=False, ds_mean=self.ds_mean, ds_std=self.ds_std)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Create train dataloader object with drop_last flag set to True.\n",
    "        return DataLoader(\n",
    "            self.train_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory,\n",
    "            num_workers=self.num_workers, drop_last=True, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Create validation dataloader object.\n",
    "        return DataLoader(\n",
    "            self.valid_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory,\n",
    "            num_workers=self.num_workers, shuffle=self.shuffle_validation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3abb40b2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.887Z"
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1713965438643,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "3abb40b2"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "#dm = MedicalSegmentationDataModule(\n",
    "#    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "#    img_size=DatasetConfig.IMAGE_SIZE,\n",
    "#    ds_mean=DatasetConfig.MEAN,\n",
    "#    ds_std=DatasetConfig.STD,\n",
    "#    batch_size=InferenceConfig.BATCH_SIZE,\n",
    "#    num_workers=0,\n",
    "#    shuffle_validation=True,\n",
    "#)\n",
    "\n",
    "# Donwload dataset.\n",
    "#dm.prepare_data()\n",
    "\n",
    "# Create training & validation dataset.\n",
    "#dm.setup()\n",
    "\n",
    "#train_loader, valid_loader = dm.train_dataloader(), dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccdaabfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:57:11.921526Z",
     "start_time": "2023-07-14T07:57:11.825989Z"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1713965442370,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "ccdaabfe"
   },
   "outputs": [],
   "source": [
    "def num_to_rgb(num_arr, color_map=id2color):\n",
    "    single_layer = np.squeeze(num_arr)\n",
    "    output = np.zeros(num_arr.shape[:2] + (3,))\n",
    "\n",
    "    for k in color_map.keys():\n",
    "        output[single_layer == k] = color_map[k]\n",
    "\n",
    "    # return a floating point array in range [0.0, 1.0]\n",
    "    return np.float32(output) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03897833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:57:11.921526Z",
     "start_time": "2023-07-14T07:57:11.825989Z"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1713965443945,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "03897833"
   },
   "outputs": [],
   "source": [
    "# Function to overlay a segmentation map on top of an RGB image.\n",
    "def image_overlay(image, segmented_image):\n",
    "    alpha = 1.0  # Transparency for the original image.\n",
    "    beta = 0.7  # Transparency for the segmentation map.\n",
    "    gamma = 0.0  # Scalar added to each sum.\n",
    "\n",
    "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    image = cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return np.clip(image, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8c56e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:57:11.921526Z",
     "start_time": "2023-07-14T07:57:11.825989Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713965456262,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "a8c56e4c"
   },
   "outputs": [],
   "source": [
    "def denormalize(tensors, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    for c in range(3):\n",
    "        tensors[:, c, :, :].mul_(std[c]).add_(mean[c])\n",
    "\n",
    "    return torch.clamp(tensors, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5fd0ea5",
   "metadata": {
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1713965467664,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "a5fd0ea5"
   },
   "outputs": [],
   "source": [
    "def get_model(*, model_name, num_classes):\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec06e142",
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1713965490343,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "ec06e142"
   },
   "outputs": [],
   "source": [
    "class MedicalSegmentationModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_classes: int = 10,\n",
    "        init_lr: float = 0.001,\n",
    "        optimizer_name: str = \"Adam\",\n",
    "        weight_decay: float = 1e-4,\n",
    "        use_scheduler: bool = False,\n",
    "        scheduler_name: str = \"multistep_lr\",\n",
    "        num_epochs: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save the arguments as hyperparameters.\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Loading model using the function defined above.\n",
    "        self.model = get_model(model_name=self.hparams.model_name, num_classes=self.hparams.num_classes)\n",
    "\n",
    "        # Initializing the required metric objects.\n",
    "        self.mean_train_loss = MeanMetric()\n",
    "        self.mean_train_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "        self.mean_valid_loss = MeanMetric()\n",
    "        self.mean_valid_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        outputs = self.model(pixel_values=data, return_dict=True)\n",
    "        upsampled_logits = F.interpolate(outputs[\"logits\"], size=data.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return upsampled_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24dacc05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1713965496076,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "24dacc05",
    "outputId": "9c6eee87-3eb5-405a-d720-668ea35448e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([4, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Seed everything for reproducibility.\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "model = MedicalSegmentationModel(\n",
    "    model_name=TrainingConfig.MODEL_NAME,\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    init_lr=TrainingConfig.INIT_LR,\n",
    "    optimizer_name=TrainingConfig.OPTIMIZER_NAME,\n",
    "    weight_decay=TrainingConfig.WEIGHT_DECAY,\n",
    "    use_scheduler=TrainingConfig.USE_SCHEDULER,\n",
    "    scheduler_name=TrainingConfig.SCHEDULER,\n",
    "    num_epochs=TrainingConfig.NUM_EPOCHS,\n",
    ")\n",
    "\n",
    "data_module = MedicalSegmentationDataModule(\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    img_size=DatasetConfig.IMAGE_SIZE,\n",
    "    ds_mean=DatasetConfig.MEAN,\n",
    "    ds_std=DatasetConfig.STD,\n",
    "    batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    num_workers=TrainingConfig.NUM_WORKERS,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a684fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = 'models/ckpt_epoch=049-vloss_val_loss=0.0000_vf1_valid_f1=0.0000.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11705a57-12ba-43db-97c4-1983acb4cffc",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1713965038262,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "11705a57-12ba-43db-97c4-1983acb4cffc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([4, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = MedicalSegmentationModel.load_from_checkpoint(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec744601",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1713965038262,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "ec744601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n",
      "[False False False False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Get the validation dataloader.\n",
    "\n",
    "#data_module.setup()\n",
    "\n",
    "data_module.setup(classif_labels=CLASSIF_LABELS)\n",
    "valid_loader = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "60294fe7-c252-4c13-81bf-4ed210306cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dice coefficient\n",
    "def dice_coef(predictions, ground_truths, num_classes=4, dims=(1, 2), smooth=1e-8):\n",
    "    \"\"\"Smooth Dice coefficient\"\"\"\n",
    "    # (batch_size, num_classes, height, width)\n",
    "    ground_truth_oh = F.one_hot(ground_truths, num_classes=num_classes)\n",
    "    # (batch_size, num_classes, height, width)\n",
    "    prediction_norm = F.one_hot(predictions, num_classes=num_classes)\n",
    "\n",
    "    # (batch_size, num_classes)\n",
    "    intersection = (prediction_norm * ground_truth_oh).sum(dim=dims)\n",
    "    # (batch_size, num_classes)\n",
    "    summation = prediction_norm.sum(dim=dims) + ground_truth_oh.sum(dim=dims)\n",
    "    # (batch_size, num_classes)\n",
    "    dice = (2.0 * intersection + smooth) / (summation + smooth)\n",
    "    dice_mean = dice.mean()\n",
    "\n",
    "    return dice_mean\n",
    "\n",
    "def detect_nonzero_masks(masks):\n",
    "    # Check if any element is nonzero along the last two axes\n",
    "    nonzero_masks_mask = np.any(masks != 0, axis=(1, 2))\n",
    "    # Get the indices of the zero images\n",
    "    nonzero_masks_indices = np.where(nonzero_masks_mask)[0]\n",
    "    return nonzero_masks_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "264e090d-cdb4-431a-9dc1-26833f43d951",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1713965038262,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "264e090d-cdb4-431a-9dc1-26833f43d951"
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def inference(model, loader, img_size, device=\"cpu\"):\n",
    "    num_batches_to_process = InferenceConfig.NUM_BATCHES\n",
    "\n",
    "    cont = 1\n",
    "    cont2 = 1\n",
    "    score_sum = 0\n",
    "    score_sum2 = 0\n",
    "    for idx, (batch_img, batch_mask, batch_labels) in enumerate(loader):\n",
    "\n",
    "        #print(batch_mask.shape)\n",
    "        print(batch_labels)\n",
    "        # Make predictions\n",
    "        predictions = model(batch_img.to(device))\n",
    "        pred_all = predictions.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Re-label to zero the masks that have been identified as Class 0 by the classifier.\n",
    "        pred_all[np.where(batch_labels.numpy().copy() == False)] = 0\n",
    "    \n",
    "        # Plot results\n",
    "        #batch_img = denormalize(batch_img.cpu(), mean=DatasetConfig.MEAN, std=DatasetConfig.STD)\n",
    "        #batch_img = batch_img.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "        if idx == num_batches_to_process * 20:\n",
    "            break\n",
    "            \n",
    "        score = dice_coef(torch.tensor(pred_all),batch_mask).numpy()\n",
    "        score_sum = score_sum + score\n",
    "        score_ave = score_sum / cont\n",
    "        cont = cont + 1\n",
    "        print(np.round(score,3), np.round(score_ave,3))\n",
    "\n",
    "        nonzero_mask_idxs = detect_nonzero_masks(batch_mask.numpy())\n",
    "\n",
    "        if len(nonzero_mask_idxs) > 0:\n",
    "            print(nonzero_mask_idxs)\n",
    "            score2 = dice_coef(torch.tensor(pred_all[nonzero_mask_idxs]),batch_mask[nonzero_mask_idxs]).numpy()\n",
    "            score_sum2 = score_sum2 + score2\n",
    "            score_ave2 = score_sum2 / cont2\n",
    "            cont2 = cont2 + 1\n",
    "            print(np.round(score2,3), np.round(score_ave2,3))\n",
    "           \n",
    "#        for i in range(0, len(batch_img)):\n",
    "#            fig = plt.figure(figsize=(20, 8))\n",
    "#\n",
    "#            # Display the original image.\n",
    "#            ax1 = fig.add_subplot(1, 4, 1)\n",
    "#            ax1.imshow(batch_img[i])\n",
    "#            ax1.title.set_text(\"Actual frame\")\n",
    "#            plt.axis(\"off\")\n",
    "\n",
    "            # Display the ground truth mask.\n",
    "#            true_mask_rgb = num_to_rgb(batch_mask[i], color_map=id2color)\n",
    "#            ax2 = fig.add_subplot(1, 4, 2)\n",
    "#            ax2.set_title(\"Ground truth labels\")\n",
    "#            ax2.imshow(true_mask_rgb)\n",
    "#            plt.axis(\"off\")\n",
    "\n",
    "            # Display the predicted segmentation mask.\n",
    "#            pred_mask_rgb = num_to_rgb(pred_all[i], color_map=id2color)\n",
    "#            ax3 = fig.add_subplot(1, 4, 3)\n",
    "#            ax3.set_title(\"Predicted labels\")\n",
    "#            ax3.imshow(pred_mask_rgb)\n",
    "#            plt.axis(\"off\")\n",
    "\n",
    "            # Display the predicted segmentation mask overlayed on the original image.\n",
    "#            overlayed_image = image_overlay(batch_img[i], pred_mask_rgb)\n",
    "#            ax4 = fig.add_subplot(1, 4, 4)\n",
    "#            ax4.set_title(\"Overlayed image\")\n",
    "#            ax4.imshow(overlayed_image)\n",
    "#            plt.axis(\"off\")\n",
    "#            plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "13bc4110-a11b-40fc-bf4b-4584fa30e7c5",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1713965038262,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "13bc4110-a11b-40fc-bf4b-4584fa30e7c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "1.0 1.0\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "1.0 1.0\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False])\n",
      "0.958 0.986\n",
      "[11]\n",
      "0.75 0.75\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "0.977 0.984\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "0.977 0.863\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "0.962 0.98\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "0.962 0.896\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "0.933 0.972\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "0.933 0.906\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "0.925 0.965\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "0.925 0.909\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "0.955 0.964\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "0.955 0.917\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True])\n",
      "0.949 0.962\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "0.949 0.922\n",
      "tensor([False, False,  True,  True,  True,  True, False,  True,  True, False,\n",
      "        False, False])\n",
      "0.817 0.948\n",
      "[0 1 2 3 4 5]\n",
      "0.802 0.907\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "1.0 0.952\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n",
      "1.0 0.956\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 6\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDatasetConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[151], line 14\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, loader, img_size, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_labels)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m pred_all \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Re-label to zero the masks that have been identified as Class 0 by the classifier.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 28\u001b[0m, in \u001b[0;36mMedicalSegmentationModel.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m---> 28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     upsampled_logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m], size\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m upsampled_logits\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:791\u001b[0m, in \u001b[0;36mSegformerForSemanticSegmentation.forward\u001b[1;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    786\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    787\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    788\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    789\u001b[0m )\n\u001b[1;32m--> 791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# we need the intermediate hidden states\u001b[39;49;00m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    800\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_head(encoder_hidden_states)\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:548\u001b[0m, in \u001b[0;36mSegformerModel.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    543\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    544\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    545\u001b[0m )\n\u001b[0;32m    546\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 548\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:425\u001b[0m, in \u001b[0;36mSegformerEncoder.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# second, send embeddings through blocks\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(block_layer):\n\u001b[1;32m--> 425\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:346\u001b[0m, in \u001b[0;36mSegformerLayer.forward\u001b[1;34m(self, hidden_states, height, width, output_attentions)\u001b[0m\n\u001b[0;32m    343\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(attention_output)\n\u001b[0;32m    344\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attention_output \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m--> 346\u001b[0m mlp_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# second residual connection (with stochastic depth)\u001b[39;00m\n\u001b[0;32m    349\u001b[0m mlp_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(mlp_output)\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\transformers\\models\\segformer\\modeling_segformer.py:305\u001b[0m, in \u001b[0;36mSegformerMixFFN.forward\u001b[1;34m(self, hidden_states, height, width)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, height, width):\n\u001b[1;32m--> 305\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdwconv(hidden_states, height, width)\n\u001b[0;32m    307\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "inference(model, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e1359-3862-40bc-a762-52c67429fe1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299.896px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1378ba22f96e4134a3aadf23f395c097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50cd2c575f6345f5a159bfc600484d0e",
       "IPY_MODEL_86f3315ff10144f095aea40c461d256e",
       "IPY_MODEL_1700c995be8b45e2ae7676ae73dc88c5"
      ],
      "layout": "IPY_MODEL_3552cf8af21745129c9750e6ca619f29"
     }
    },
    "1700c995be8b45e2ae7676ae73dc88c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84878029a4fe45798f0c4728b738652d",
      "placeholder": "​",
      "style": "IPY_MODEL_4467c1d174224764bf4c0d4a8abe8dca",
      "value": " 2/2 [00:00&lt;00:00,  4.49it/s]"
     }
    },
    "17038ea0beb5498c8ef4b792cc4f78f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "178ab317e5ae44ad8640feb4c4db4837": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "23d8be2f4b884a0c9544b546a292f9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_714473c7d20848b6a742abfea94f3c52",
      "max": 280,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec5bbe8822bd4b34a784726a7671bca6",
      "value": 280
     }
    },
    "24fdc15d41624e5682b2e8322ea3a3f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a54a04435044109a737ba66ade2aafc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3552cf8af21745129c9750e6ca619f29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "4467c1d174224764bf4c0d4a8abe8dca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49140305690348abb2a551d3bdd3849e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24fdc15d41624e5682b2e8322ea3a3f0",
      "placeholder": "​",
      "style": "IPY_MODEL_738e6f0aea304c50a039395809b48216",
      "value": "Epoch 0: 100%"
     }
    },
    "50cd2c575f6345f5a159bfc600484d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a54a04435044109a737ba66ade2aafc",
      "placeholder": "​",
      "style": "IPY_MODEL_c195462a99934f639c166547547f433c",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "6647710a025145c09dc750f2dbf889bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f3f92ccc6324c1bbc1e4458b54ba521",
       "IPY_MODEL_23d8be2f4b884a0c9544b546a292f9bc",
       "IPY_MODEL_85d41b54e32d48a789c88379eb74d329"
      ],
      "layout": "IPY_MODEL_6c1a05d1bcb94ab5993b861b1a9b9c00"
     }
    },
    "6c1a05d1bcb94ab5993b861b1a9b9c00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "714473c7d20848b6a742abfea94f3c52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "738e6f0aea304c50a039395809b48216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76c71109197449199fe6b5126cbfdfcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8297e891e764434cb1c8007e6adb9fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84878029a4fe45798f0c4728b738652d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85d41b54e32d48a789c88379eb74d329": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe66361110e5430fb8e721025f6c7e75",
      "placeholder": "​",
      "style": "IPY_MODEL_76c71109197449199fe6b5126cbfdfcd",
      "value": " 280/280 [01:37&lt;00:00,  2.87it/s]"
     }
    },
    "86f3315ff10144f095aea40c461d256e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e654fa4cd4314f7a9f070558a7119659",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9a2e9b44f294265a9c7cacf4678d999",
      "value": 2
     }
    },
    "9f3f92ccc6324c1bbc1e4458b54ba521": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c306eacaeb3b47019480caf4a0467a03",
      "placeholder": "​",
      "style": "IPY_MODEL_d134b87624894ab4946ec1c07f0fa48d",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "be22a7e68186419f8bb2b1f6ff525402": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c195462a99934f639c166547547f433c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c306eacaeb3b47019480caf4a0467a03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9a2e9b44f294265a9c7cacf4678d999": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d134b87624894ab4946ec1c07f0fa48d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1d57cb119c644148b5ccaeb79dd60cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17038ea0beb5498c8ef4b792cc4f78f0",
      "placeholder": "​",
      "style": "IPY_MODEL_f3d9346390a4463c8d916d58101e290d",
      "value": " 1103/1103 [19:22&lt;00:00,  0.95it/s, v_num=zws9, train/batch_loss=nan.0, train/batch_f1=nan.0, valid/loss=nan.0, valid/f1=0.245]"
     }
    },
    "e59fc8cc7f784fceaf205d97685d263d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49140305690348abb2a551d3bdd3849e",
       "IPY_MODEL_f05896a64ca640d085018265a319cec3",
       "IPY_MODEL_e1d57cb119c644148b5ccaeb79dd60cf"
      ],
      "layout": "IPY_MODEL_178ab317e5ae44ad8640feb4c4db4837"
     }
    },
    "e654fa4cd4314f7a9f070558a7119659": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec5bbe8822bd4b34a784726a7671bca6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f05896a64ca640d085018265a319cec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be22a7e68186419f8bb2b1f6ff525402",
      "max": 1103,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8297e891e764434cb1c8007e6adb9fa4",
      "value": 1103
     }
    },
    "f3d9346390a4463c8d916d58101e290d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe66361110e5430fb8e721025f6c7e75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
