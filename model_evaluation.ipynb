{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb1234d-1269-493e-a1bc-88771886e354",
   "metadata": {},
   "source": [
    "# Notebook on Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "202633d9-7068-4196-8458-21d49a0ab9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score, roc_curve\n",
    "from keras.models import load_model\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae63c577-d52b-4372-a64f-46f2f6f0c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your DataFrame loading and regex patterns\n",
    "#df = pd.read_csv('data/train.csv')\n",
    "#test_cases = ['2', '6', '7', '9', '11', '15', '16', '140', '145', '146', '147', '148', '149', '154', '156']\n",
    "#GET_CASE = re.compile(r\"case[0-9]{1,3}\")\n",
    "#GET_CASE_AND_DATE = re.compile(r\"case[0-9]{1,3}_day[0-9]{1,3}\")\n",
    "#GET_SLICE_NUM = re.compile(r\"slice_[0-9]{1,4}\")\n",
    "#IMG_SHAPE = re.compile(r\"_[0-9]{1,3}_[0-9]{1,3}_\")\n",
    "#\n",
    "#df_out = pd.DataFrame(columns=df.columns)#\n",
    "#\n",
    "#def take_test_cases(row):\n",
    "#    # Iterate over the rows of the DataFrame\n",
    "#    case = GET_CASE.search(row[\"id\"])[0][4:]  # Assuming the case number is in the first column\n",
    "#    if case in test_cases:\n",
    "#        #print(row)\n",
    "#        return [row['id'], row['class'], row['segmentation']]\n",
    "#    else:\n",
    "#        #return [np.NaN, np.NaN, np.NaN]\n",
    "#        pass\n",
    "#\n",
    "##df_out[['id', 'class', 'segmentation']] = df.apply(take_test_cases, axis=1, result_type='expand')\n",
    "##df_out.dropna().to_csv('tmp.csv')\n",
    "#df_out = df.apply(take_test_cases, axis=1, result_type='expand')\n",
    "#df_out.to_csv('tmp.csv')\n",
    "##df_out.dropna(axis=0)#df_out.head()\n",
    "## Use boolean indexing to select rows where the first column matches any element in test_cases\n",
    "##mask = df.iloc[:, 0].str.extract(GET_CASE)[0][4:].isin(test_cases)\n",
    "##mask\n",
    "##mask = df.iloc[:, 0].str.extract(GET_CASE)[0].str[0].isin(test_cases)\n",
    "##df_out = df[mask]\n",
    "# Print the resulting DataFrame\n",
    "#print(df_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d243b-be2f-4c93-a668-e03ec33fbb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdb334b-5ef6-43d7-befe-b051d7323deb",
   "metadata": {},
   "source": [
    "# Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077a6c49-51d9-46af-b685-3ce92a62155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and constants\n",
    "MEAN = 0.136\n",
    "STD = 0.178\n",
    "IM_SIZE = 288\n",
    "BATCH_SIZE_CLF = 32\n",
    "BATCH_SIZE_SGM = 2\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    BATCH_SIZE:  int = 10\n",
    "    NUM_BATCHES: int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0cc229-1e13-4d0a-a5af-f642aebf2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input directory\n",
    "ROOT_DIR = \"2p5d_stride1_all_v2\"\n",
    "IMG_DIR = os.path.join(ROOT_DIR, \"valid\")\n",
    "MSK_DIR = os.path.join(ROOT_DIR, \"valid\", \"masks\")\n",
    "#IMG_DIR = \"2p5d_stride1_all_v2/valid/images\"\n",
    "#MASK_DIR = \"2p5d_stride1_all_v2/valid/masks\"\n",
    "#CASE_FOLDERS = os.listdir(IMG_DIR)\n",
    "MODEL_DIR = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5bc9f-dbc8-4d3f-a8df-54ff9e43f052",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af649d82-1c2c-4ebc-b508-ea309d9196f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted segmentation masks, typically output by a segmentation model.\n",
    "# It's expected to have a shape of (batch_size, height, width, num_classes)\n",
    "def dice_coef(predictions, ground_truths, num_classes=2, dims=(1, 2), smooth=1e-8):\n",
    "    \"\"\"Smooth Dice coefficient\"\"\"\n",
    "\n",
    "    # (batch_size, num_classes, height, width).\n",
    "    ground_truth_oh = F.one_hot(ground_truths, num_classes=num_classes)\n",
    "    # (batch_size, height, width, num_classes)\n",
    "    # permute(0, 2, 3, 1) to change the dimensions to match the one-hot\n",
    "    # encoded ground truth for element-wise multiplication in the next step.\n",
    "    prediction_norm = F.softmax(predictions, dim=1).permute(0, 2, 3, 1)\n",
    "\n",
    "    # (batch_size, num_classes)\n",
    "    intersection = (prediction_norm * ground_truth_oh).sum(dim=dims)\n",
    "    # (batch_size, num_classes)\n",
    "    summation = prediction_norm.sum(dim=dims) + ground_truth_oh.sum(dim=dims)\n",
    "    # (batch_size, num_classes)\n",
    "    dice = (2.0 * intersection + smooth) / (summation + smooth)\n",
    "    dice_mean = dice.mean()\n",
    "\n",
    "    return dice_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24df0ef-5a0d-4222-8c6c-56d453d7fc84",
   "metadata": {},
   "source": [
    "# Model 1: Classification + Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd923b5b-1cf9-452a-9bbe-b9ee3711b147",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb210e02-0486-489b-99fa-2f59b5030f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classification model\n",
    "class_model_name = os.path.join(MODEL_DIR, \"resnet50v2_nn256_lr0001_relu_batch32_epoch30_v2_2p5d.keras\")\n",
    "class_model = load_model(class_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6851d8-67f3-444d-845b-8b3bdbce796f",
   "metadata": {},
   "source": [
    "## Prepare Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbfcd133-53a2-4a08-bf17-a6c47bd61bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2p5d_stride1_all_v2\\valid\n",
      "Found 6560 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Function for custom normalization\n",
    "def custom_normalization(image):  \n",
    "    image = image / 255.0\n",
    "    image = (image - MEAN) / STD\n",
    "    return image\n",
    "    \n",
    "# Image preprocessing\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = custom_normalization)\n",
    "\n",
    "# Define the test set\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    IMG_DIR,\n",
    "    target_size=(IM_SIZE, IM_SIZE),\n",
    "    batch_size=BATCH_SIZE_CLF,\n",
    "    #class_mode=None,\n",
    "    classes=['images'],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac41c1e-7797-4979-bf28-853b454efc60",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf35349-adc6-4c64-8655-4c67b69cd87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergio\\.venv_common\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7168s\u001b[0m 35s/step\n"
     ]
    }
   ],
   "source": [
    "#images = load_images(IMG_DIR, (IM_SIZE, IM_SIZE))\n",
    "#y_pred = class_model.predict(images).reshape(-1)\n",
    "y_pred = class_model.predict(test_generator).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d773c9d-017d-4682-a040-833ef1f38259",
   "metadata": {},
   "outputs": [],
   "source": [
    "THR = 0.1586085855960846\n",
    "MASK = y_pred > THR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61218274-d1ab-479b-828d-a6ce8247c196",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=static/ckpt_009.ckpt. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(static/ckpt_009.ckpt, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# load the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic/ckpt_009.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# predict!\u001b[39;00m\n\u001b[0;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m inference(model, image_paths_clf1, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:191\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File format not supported: filepath=static/ckpt_009.ckpt. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(static/ckpt_009.ckpt, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "# store in list the two image paths correspoding to the classification output\n",
    "image_list = os.listdir(os.path.join(IMG_DIR, \"images\"))\n",
    "image_paths_clf1 = [b for a, b in zip(MASK, image_list) if a]\n",
    "image_paths_clf0 = [b for a, b in zip(MASK, image_list) if not(a)]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the model\n",
    "checkpoint_path = \"static/ckpt_009.ckpt\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "# predict!\n",
    "predictions = inference(model, image_paths_clf1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb6fd4-c6e5-4077-8a39-6a7c07d94927",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def inference(model, image_paths, device=\"cpu\"):\n",
    "    batch_size = InferenceConfig.BATCH_SIZE\n",
    "    num_images = len(image_paths)\n",
    "    num_batches_to_process = (num_images + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    #overlay_paths = []\n",
    "    for batch_idx in range(num_batches_to_process):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(image_paths))\n",
    "        batch_images = []\n",
    "        for path in image_paths[start_idx:end_idx]:\n",
    "            # Load image from path\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            # Convert image to numpy array\n",
    "            image_np = np.array(image)\n",
    "            # Convert image to tensor -- and make sure a float and not double!!\n",
    "            image_tensor = torch.tensor(image_np / 255.0, dtype=torch.float).permute(2, 0, 1).to(device)\n",
    "            batch_images.append(image_tensor)\n",
    "        batch_images = torch.stack(batch_images)\n",
    "        model.eval()\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            predictions = model(batch_images)\n",
    "        pred_all = predictions.logits.argmax(dim=1)\n",
    "\n",
    "        # He we can compute the Dice_coeff and accumulate\n",
    "        \n",
    "        # Append batch of predicted masks to the list\n",
    "        all_predictions.append(pred_all.cpu().numpy())\n",
    "\n",
    "    # Concatenate all predicted masks into a single NumPy array\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "    return all_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_common",
   "language": "python",
   "name": ".venv_common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
