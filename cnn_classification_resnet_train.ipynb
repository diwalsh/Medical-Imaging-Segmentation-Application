{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Stomach or Intestine in Medial Images Using ResNet50V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import ResNet50, ResNet50V2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score, roc_curve\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.models import load_model, Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#import subprocess as sp\n",
    "#\n",
    "#def mask_unused_gpus(leave_unmasked=1):\n",
    "#  ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "#  COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "#  try:\n",
    "#    _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "#    memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "#    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "#    available_gpus = [i for i, x in enumerate(memory_free_values) if x > ACCEPTABLE_AVAILABLE_MEMORY]\n",
    "#\n",
    "#    if len(available_gpus) < leave_unmasked: raise ValueError('Found only %d usable GPUs in the system' % len(available_gpus))\n",
    "#    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, available_gpus[:leave_unmasked]))\n",
    "#  except Exception as e:\n",
    "#    print('\"nvidia-smi\" is probably not installed. GPUs are not masked', e)\n",
    "#\n",
    "#mask_unused_gpus(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and constants\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARN_RATE = 0.001\n",
    "\n",
    "MEAN = 0.136\n",
    "STD = 0.178\n",
    "\n",
    "IM_SIZE = 288\n",
    "\n",
    "DATASET = 'dataset_UWM_GI_Tract_classification_rgb_2p5d_test15'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for custom normalization\n",
    "def custom_normalization(image):\n",
    "    image = image / 255.0\n",
    "    image = (image - MEAN) / STD\n",
    "    return image\n",
    "\n",
    "# ploting model loss during training, created by Daniel: https://medium.com/geekculture/how-to-plot-model-loss-while-training-in-tensorflow-9fa1a1875a5\n",
    "class plot_learning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "\n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='val_' + metric)\n",
    "                \n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def find_roc_threshold_tpr(y, y_pred, value_target):\n",
    "    \"\"\"\n",
    "    This function calculates the threshold and false positive rate corresponding to a true positive rate of value_target (from 0 to 1).\n",
    "       \n",
    "    y                     # Real labels\n",
    "    y_pred                # Predicted labels\n",
    "    value_target          # False positive rate value\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    threshold             # Threshold value\n",
    "    true_positive_rate   # True positive rate value\n",
    "    \"\"\"\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y, y_pred)\n",
    "\n",
    "    old_diff = 100000000\n",
    "    for index, value in enumerate(tpr):\n",
    "        new_diff = abs(value_target - value)\n",
    "        if new_diff < old_diff:\n",
    "            false_pos_rate = fpr[index]\n",
    "            threshold = thr[index]\n",
    "            old_diff = new_diff\n",
    "\n",
    "    return threshold, false_pos_rate\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges,\n",
    "                          figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    # Confusion matrix\n",
    "    #cm = confusion_matrix(test_labels, rf_predictions)\n",
    "    #plot_confusion_matrix(cm, classes = ['Poor Health', 'Good Health'],\n",
    "    #                      title = 'Health Confusion Matrix')\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    " \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    #plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25550 images belonging to 2 classes.\n",
      "Found 6386 images belonging to 2 classes.\n",
      "Found 6560 images belonging to 2 classes.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssre_\\.venv_common\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 45/799\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18:00\u001b[0m 1s/step - accuracy: 0.6575 - loss: 9.0272 - precision: 0.6021 - recall: 0.6084"
     ]
    }
   ],
   "source": [
    "# Preprocess the images and create data generators with augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = custom_normalization,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.5, 1.5],  # Adjust brightness randomly between 0.5 and 1.5\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = custom_normalization)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET + '/train',\n",
    "    target_size=(IM_SIZE, IM_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    DATASET + '/train',\n",
    "    target_size=(IM_SIZE, IM_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    DATASET + '/test',\n",
    "    target_size=(IM_SIZE, IM_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the pre-trained ResNet50 model\n",
    "#wgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
    "#resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(IM_SIZE, IM_SIZE, 3))\n",
    "resnet_base = ResNet50V2(weights='imagenet', include_top=False, input_shape=(IM_SIZE, IM_SIZE, 3))\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Get the output tensor of the base ResNet50V2 model\n",
    "resnet_output = resnet_base.output\n",
    "\n",
    "# Flatten the output tensor\n",
    "flattened_output = Flatten()(resnet_output)\n",
    "\n",
    "# Add the fully connected layers: 1 layer with 256 neurons and the output layer with a sigmoid function\n",
    "deep_layers = Dense(256, kernel_initializer=glorot_uniform(seed=42), activation='relu')(flattened_output)\n",
    "dropout = Dropout(0.5)(deep_layers)\n",
    "output = Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=resnet_base.input, outputs=output)\n",
    "\n",
    "# Compile de model\n",
    "optimizer = Adam(learning_rate=LEARN_RATE)\n",
    "eval_metrics = [tf.keras.metrics.Precision(thresholds=0.5), tf.keras.metrics.Recall(thresholds=0.5), \"accuracy\"]\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=eval_metrics)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    batch_size=BATCH_SIZE,    \n",
    "    epochs=EPOCHS,\n",
    "    #validation steps=\n",
    "    #steps_per_epoch\n",
    "    verbose=1,\n",
    "    validation_data=validation_generator,    \n",
    "    #class_weight=class_weight,\n",
    "    callbacks=plot_learning()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "#model.save(\"resnet50v2_nn256_lr0001_relu_batch32_epoch20.keras\")\n",
    "#model.save(\"resnet50v2_nn256_lr0001_relu_batch32_epoch30.keras\")\n",
    "model.save(\"resnet50v2_nn256_lr0001_relu_batch32_epoch50_v3_2p5d.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_common",
   "language": "python",
   "name": ".venv_common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
