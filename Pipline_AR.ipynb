{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in ./.venv/lib/python3.11/site-packages (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.24.4 in ./.venv/lib/python3.11/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.11/site-packages (from albumentations) (1.13.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in ./.venv/lib/python3.11/site-packages (from albumentations) (0.23.1)\n",
      "Requirement already satisfied: PyYAML in ./.venv/lib/python3.11/site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in ./.venv/lib/python3.11/site-packages (from albumentations) (4.11.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in ./.venv/lib/python3.11/site-packages (from albumentations) (1.4.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0 in ./.venv/lib/python3.11/site-packages (from albumentations) (4.9.0.80)\n",
      "Requirement already satisfied: networkx>=2.8 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (2024.2.12)\n",
      "Requirement already satisfied: packaging>=21 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./.venv/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.3.2->albumentations) (3.4.0)\n",
      "fatal: destination path 'Kaggle-UWMGIT' already exists and is not an empty directory.\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.11/site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (8.23.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (4.66.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.venv/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.11/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: segmentation-models-pytorch in ./.venv/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in ./.venv/lib/python3.11/site-packages (from segmentation-models-pytorch) (0.17.2)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in ./.venv/lib/python3.11/site-packages (from segmentation-models-pytorch) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in ./.venv/lib/python3.11/site-packages (from segmentation-models-pytorch) (0.7.1)\n",
      "Requirement already satisfied: timm==0.9.2 in ./.venv/lib/python3.11/site-packages (from segmentation-models-pytorch) (0.9.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from segmentation-models-pytorch) (4.66.2)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.11/site-packages (from segmentation-models-pytorch) (10.3.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.2)\n",
      "Requirement already satisfied: munch in ./.venv/lib/python3.11/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.11/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.22.2)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.11/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (23.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.11/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# requirements\n",
    "\n",
    "!pip install albumentations\n",
    "!git clone https://github.com/CarnoZhao/Kaggle-UWMGIT && cd Kaggle-UWMGIT && pip install -e .\n",
    "!pip install ipywidgets\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Windows\n",
    "!pip3 install torch torchvision torchaudio # Mac\n",
    "!pip install segmentation-models-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Third-party libraries for data handling and computation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing and augmentation libraries\n",
    "import cv2\n",
    "from albumentations import Compose, Normalize, Resize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# PyTorch and related libraries for deep learning\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Libraries for neural network models and progress visualization\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read csv and extract meta info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob  # Import the glob module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>patient</th>\n",
       "      <th>days</th>\n",
       "      <th>image_files</th>\n",
       "      <th>spacing_x</th>\n",
       "      <th>spacing_y</th>\n",
       "      <th>size_x</th>\n",
       "      <th>size_y</th>\n",
       "      <th>slice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case2_day1_slice_0001</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case2</td>\n",
       "      <td>case2_day1</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case2_day1_slice_0001</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case2</td>\n",
       "      <td>case2_day1</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case2_day1_slice_0001</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case2</td>\n",
       "      <td>case2_day1</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case2_day1_slice_0002</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case2</td>\n",
       "      <td>case2_day1</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case2_day1_slice_0002</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case2</td>\n",
       "      <td>case2_day1</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>case9_day22_slice_0143</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case9</td>\n",
       "      <td>case9_day22</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>360</td>\n",
       "      <td>310</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>case9_day22_slice_0143</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case9</td>\n",
       "      <td>case9_day22</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>360</td>\n",
       "      <td>310</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>case9_day22_slice_0144</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case9</td>\n",
       "      <td>case9_day22</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>360</td>\n",
       "      <td>310</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6046</th>\n",
       "      <td>case9_day22_slice_0144</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case9</td>\n",
       "      <td>case9_day22</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>360</td>\n",
       "      <td>310</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6047</th>\n",
       "      <td>case9_day22_slice_0144</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case9</td>\n",
       "      <td>case9_day22</td>\n",
       "      <td>uw-madison-gi-tract-image-segmentation/train_4...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>360</td>\n",
       "      <td>310</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6048 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id        class segmentation patient         days   \n",
       "0      case2_day1_slice_0001  large_bowel          NaN   case2   case2_day1  \\\n",
       "1      case2_day1_slice_0001  small_bowel          NaN   case2   case2_day1   \n",
       "2      case2_day1_slice_0001      stomach          NaN   case2   case2_day1   \n",
       "3      case2_day1_slice_0002  large_bowel          NaN   case2   case2_day1   \n",
       "4      case2_day1_slice_0002  small_bowel          NaN   case2   case2_day1   \n",
       "...                      ...          ...          ...     ...          ...   \n",
       "6043  case9_day22_slice_0143  small_bowel          NaN   case9  case9_day22   \n",
       "6044  case9_day22_slice_0143      stomach          NaN   case9  case9_day22   \n",
       "6045  case9_day22_slice_0144  large_bowel          NaN   case9  case9_day22   \n",
       "6046  case9_day22_slice_0144  small_bowel          NaN   case9  case9_day22   \n",
       "6047  case9_day22_slice_0144      stomach          NaN   case9  case9_day22   \n",
       "\n",
       "                                            image_files  spacing_x  spacing_y   \n",
       "0     uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5  \\\n",
       "1     uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "2     uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "3     uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "4     uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "...                                                 ...        ...        ...   \n",
       "6043  uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "6044  uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "6045  uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "6046  uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "6047  uw-madison-gi-tract-image-segmentation/train_4...        1.5        1.5   \n",
       "\n",
       "      size_x  size_y  slice  \n",
       "0        266     266      1  \n",
       "1        266     266      1  \n",
       "2        266     266      1  \n",
       "3        266     266      2  \n",
       "4        266     266      2  \n",
       "...      ...     ...    ...  \n",
       "6043     360     310    143  \n",
       "6044     360     310    143  \n",
       "6045     360     310    144  \n",
       "6046     360     310    144  \n",
       "6047     360     310    144  \n",
       "\n",
       "[6048 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_csv(\"data/train_4.csv\")\n",
    "df_train = df_train.sort_values([\"id\", \"class\"]).reset_index(drop = True)\n",
    "df_train[\"patient\"] = df_train.id.apply(lambda x: x.split(\"_\")[0])\n",
    "df_train[\"days\"] = df_train.id.apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "\n",
    "all_image_files = sorted(glob.glob(\"uw-madison-gi-tract-image-segmentation/train_4/*/*/scans/*.png\"), key = lambda x: x.split(\"/\")[3] + \"_\" + x.split(\"/\")[5])\n",
    "size_x = [int(os.path.basename(_)[:-4].split(\"_\")[-4]) for _ in all_image_files]\n",
    "size_y = [int(os.path.basename(_)[:-4].split(\"_\")[-3]) for _ in all_image_files]\n",
    "spacing_x = [float(os.path.basename(_)[:-4].split(\"_\")[-2]) for _ in all_image_files]\n",
    "spacing_y = [float(os.path.basename(_)[:-4].split(\"_\")[-1]) for _ in all_image_files]\n",
    "df_train[\"image_files\"] = np.repeat(all_image_files, 3)\n",
    "df_train[\"spacing_x\"] = np.repeat(spacing_x, 3)\n",
    "df_train[\"spacing_y\"] = np.repeat(spacing_y, 3)\n",
    "df_train[\"size_x\"] = np.repeat(size_x, 3)\n",
    "df_train[\"size_y\"] = np.repeat(size_y, 3)\n",
    "df_train[\"slice\"] = np.repeat([int(os.path.basename(_)[:-4].split(\"_\")[-5]) for _ in all_image_files], 3)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Make mmseg-format data (2.5D by default)\n",
    "\n",
    "### Resizing\n",
    "### Creating Mask\n",
    "### Verifing the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "def rle_decode(mask_rle, shape):\n",
    "    s = np.array(mask_rle.split(), dtype=int)\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    h, w = shape\n",
    "    img = np.zeros((h * w,), dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo: hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# Ensure the output directories exist\n",
    "output_base_path = \"./mmseg_train\"\n",
    "os.makedirs(os.path.join(output_base_path, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, \"labels\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, \"splits\"), exist_ok=True)\n",
    "\n",
    "# Verify df_train is loaded properly\n",
    "if 'df_train' not in locals():\n",
    "    print(\"DataFrame 'df_train' is not defined.\")\n",
    "    # Load or define df_train here\n",
    "else:\n",
    "    for day, group in tqdm(df_train.groupby(\"days\")):\n",
    "        patient = group.patient.iloc[0]\n",
    "        imgs = []\n",
    "        msks = []\n",
    "        file_names = []\n",
    "        \n",
    "        for file_name in group.image_files.unique():\n",
    "            img = cv2.imread(file_name, cv2.IMREAD_ANYDEPTH)\n",
    "            if img is None:\n",
    "                print(f\"Failed to read image {file_name}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            segms = group.loc[group.image_files == file_name]\n",
    "            masks = {}\n",
    "            for segm, label in zip(segms.segmentation, segms[\"class\"]):\n",
    "                if not pd.isna(segm):\n",
    "                    mask = rle_decode(segm, img.shape[:2])\n",
    "                    masks[label] = mask\n",
    "                else:\n",
    "                    masks[label] = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "            masks = np.stack([masks[k] for k in sorted(masks)], -1)\n",
    "            imgs.append(img)\n",
    "            msks.append(masks)\n",
    "        \n",
    "        if imgs and msks:\n",
    "            imgs = np.stack(imgs, 0)\n",
    "            msks = np.stack(msks, 0)\n",
    "            for i in range(msks.shape[0]):\n",
    "                img = imgs[[max(0, i - 2), i, min(imgs.shape[0] - 1, i + 2)]].transpose(1, 2, 0)  # 2.5d data\n",
    "                msk = msks[i]\n",
    "                new_file_name = f\"{day}_{i}.png\"\n",
    "                if not cv2.imwrite(f\"{output_base_path}/images/{new_file_name}\", img):\n",
    "                    print(f\"Failed to write image file: {output_base_path}/images/{new_file_name}\")\n",
    "                if not cv2.imwrite(f\"{output_base_path}/labels/{new_file_name}\", msk):\n",
    "                    print(f\"Failed to write label file: {output_base_path}/labels/{new_file_name}\")\n",
    "        else:\n",
    "            print(f\"No images or masks found for day {day}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Make fold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_files = glob.glob(\"./mmseg_train/images/*\")\n",
    "patients = [os.path.basename(_).split(\"_\")[0] for _ in all_image_files]\n",
    "\n",
    "\n",
    "split = list(GroupKFold(5).split(patients, groups = patients))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(split):\n",
    "    with open(f\"./mmseg_train/splits/fold_{fold}.txt\", \"w\") as f:\n",
    "        for idx in train_idx:\n",
    "            f.write(os.path.basename(all_image_files[idx])[:-4] + \"\\n\")\n",
    "    with open(f\"./mmseg_train/splits/holdout_{fold}.txt\", \"w\") as f:\n",
    "        for idx in valid_idx:\n",
    "            f.write(os.path.basename(all_image_files[idx])[:-4] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.058967831355172196\n",
      "Epoch 1, Loss: 0.0014153891573345939\n",
      "Epoch 2, Loss: 0.0005352417700498971\n",
      "Epoch 3, Loss: 0.0002747559075133823\n",
      "Epoch 4, Loss: 0.00016843495373789678\n",
      "Epoch 5, Loss: 0.00011382670391679904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 79\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch, model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     77\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     78\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, masks)\n\u001b[0;32m---> 79\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     81\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/repos/dats-data/.venv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/dats-data/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/dats-data/.venv/lib/python3.11/site-packages/torch/autograd/function.py:277\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augmentation=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = [img for img in os.listdir(image_dir) if img.endswith('.png')]\n",
    "        self.augmentation = augmentation\n",
    "        self.to_tensor = ToTensor()  # Converts numpy array (H x W x C) in the range [0, 255] to a torch.FloatTensor (C x H x W) in the range [0.0, 1.0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.augmentation:\n",
    "            augmented = self.augmentation(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        image = self.to_tensor(image)  # Ensure correct dimension order and scaling for model input\n",
    "        mask = torch.from_numpy(mask).long()  # Ensure mask is a long tensor\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Basic configurations\n",
    "num_classes = 3\n",
    "data_root = 'mmseg_train'\n",
    "img_size = 256\n",
    "\n",
    "# Define the model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "\n",
    "# Define training and validation data paths\n",
    "train_img_dir = os.path.join(data_root, 'images')\n",
    "train_ann_dir = os.path.join(data_root, 'labels')\n",
    "val_img_dir = os.path.join(data_root, 'images')\n",
    "val_ann_dir = os.path.join(data_root, 'labels')\n",
    "\n",
    "# Define data transformations using albumentations\n",
    "train_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_img_dir, train_ann_dir, augmentation=train_transform)\n",
    "valid_dataset = CustomDataset(val_img_dir, val_ann_dir, augmentation=val_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "def train_one_epoch(epoch, model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Start training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch, model, train_loader, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Handling gray image and RGB masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augmentation=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = [img for img in os.listdir(image_dir) if img.endswith('.png')]\n",
    "        self.augmentation = augmentation\n",
    "        self.to_tensor = ToTensor()  # Converts numpy array (H x W x C) in the range [0, 255] to a torch.FloatTensor (C x H x W) in the range [0.0, 1.0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "\n",
    "        # Load image in grayscale and expand to three channels\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Load mask in RGB\n",
    "        mask = cv2.imread(mask_path)\n",
    "\n",
    "        if self.augmentation:\n",
    "            augmented = self.augmentation(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        image = self.to_tensor(image)  # Ensure correct dimension order and scaling for model input\n",
    "        mask = torch.from_numpy(mask[:, :, 0]).long()  # Convert mask to long tensor, assuming mask is single-channel relevant info in red channel\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Basic configurations\n",
    "num_classes = 3\n",
    "data_root = 'mmseg_train'\n",
    "img_size = 256\n",
    "\n",
    "# Define the model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "\n",
    "# Define training and validation data paths\n",
    "train_img_dir = os.path.join(data_root, 'images')\n",
    "train_ann_dir = os.path.join(data_root, 'labels')\n",
    "val_img_dir = os.path.join(data_root, 'images')\n",
    "val_ann_dir = os.path.join(data_root, 'labels')\n",
    "\n",
    "# Define data transformations using albumentations\n",
    "train_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_img_dir, train_ann_dir, augmentation=train_transform)\n",
    "valid_dataset = CustomDataset(val_img_dir, val_ann_dir, augmentation=val_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "def train_one_epoch(epoch, model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Start training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch, model, train_loader, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 saving model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the entire model\n",
    "torch.save(model, '/Users/arahjou/Downloads/uw_madison/mmseg_train/model/model.pth')\n",
    "\n",
    "# Loading the entire model\n",
    "model = torch.load('/Users/arahjou/Downloads/uw_madison/mmseg_train/model/model.pth')\n",
    "model.eval()\n",
    "\n",
    "# Saving the state dictionary\n",
    "torch.save(model.state_dict(), '/Users/arahjou/Downloads/uw_madison/mmseg_train/model/model_state_dict.pth')\n",
    "num_classes = 3\n",
    "# Loading the state dictionary\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=None,  # Set to None to not load default pretrained weights\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "model.load_state_dict(torch.load('/Users/arahjou/Downloads/uw_madison/mmseg_train/model/model_state_dict.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 (saving each cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}, Loss: {average_loss}\")\n",
    "    return average_loss\n",
    "\n",
    "# Start training and save the model at the end of each epoch\n",
    "num_epochs = 10\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_one_epoch(epoch, model, train_loader, optimizer)\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), f'/Users/arahjou/Downloads/uw_madison/model/best_model_epoch_{epoch}.pth')\n",
    "        print(f\"Saved Best Model at Epoch {epoch} with Loss {best_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Using Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Your Pre-trained Model\n",
    "num_classes = 3\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=None,  # Assuming you are loading your custom trained weights\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "model.load_state_dict(torch.load('/Users/arahjou/Downloads/uw_madison/model/model_state_dict.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the Image\n",
    "# Define the image size\n",
    "img_size = 266  # Assuming the size of the image you want\n",
    "\n",
    "# Define the transformation using only albumentations\n",
    "transform = Compose([\n",
    "    Resize(height=img_size, width=img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_and_transform_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"No image found at {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transformed = transform(image=image)  # Apply the transformations\n",
    "    image_tensor = transformed['image'].unsqueeze(0)  # Add batch dimension\n",
    "    return image_tensor\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/arahjou/Downloads/uw_madison/train/case9/case9_day22/scans/slice_0071_360_310_1.50_1.50.png'\n",
    "image_tensor = load_and_transform_image(image_path)\n",
    "print(image_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Perform Inference\n",
    "with torch.no_grad():  # Turn off gradients to speed up this part\n",
    "    output = model(image_tensor)\n",
    "    prediction = torch.argmax(output, dim=1)  # Get the most likely class for each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Post-process the Output\n",
    "predicted_mask = prediction.squeeze().cpu().numpy()  # Remove batch dimension and convert to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize the Results\n",
    "\n",
    "plt.imshow(predicted_mask, cmap='gray')  # Assuming the mask is grayscale\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
