{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from albumentations) (1.12.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from albumentations) (0.23.1)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from albumentations) (4.9.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from albumentations) (1.4.1.post1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from albumentations) (4.9.0.80)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (3.1)\n",
      "Requirement already satisfied: pillow>=9.1 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (10.2.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2024.2.12)\n",
      "Requirement already satisfied: packaging>=21 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from scikit-learn>=1.3.2->albumentations) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from scikit-learn>=1.3.2->albumentations) (3.4.0)\n",
      "fatal: destination path 'Kaggle-UWMGIT' already exists and is not an empty directory.\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (4.66.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: segmentation-models-pytorch in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from segmentation-models-pytorch) (0.17.2)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from segmentation-models-pytorch) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from segmentation-models-pytorch) (0.7.1)\n",
      "Requirement already satisfied: timm==0.9.2 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from segmentation-models-pytorch) (0.9.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from segmentation-models-pytorch) (4.66.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from segmentation-models-pytorch) (10.2.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.2)\n",
      "Requirement already satisfied: munch in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.22.2)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (23.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/envs/.C_Conda/lib/python3.12/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# requirments\n",
    "\n",
    "!pip install albumentations\n",
    "!git clone https://github.com/CarnoZhao/Kaggle-UWMGIT && cd Kaggle-UWMGIT && pip install -e .\n",
    "!pip install ipywidgets\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Windows\n",
    "!pip3 install torch torchvision torchaudio # Mac\n",
    "!pip install segmentation-models-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Third-party libraries for data handling and computation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing and augmentation libraries\n",
    "import cv2\n",
    "from albumentations import Compose, Normalize, Resize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# PyTorch and related libraries for deep learning\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Libraries for neural network models and progress visualization\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read csv and extract meta info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>patient</th>\n",
       "      <th>days</th>\n",
       "      <th>image_files</th>\n",
       "      <th>spacing_x</th>\n",
       "      <th>spacing_y</th>\n",
       "      <th>size_x</th>\n",
       "      <th>size_y</th>\n",
       "      <th>slice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case20_day0_slice_0001</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case20</td>\n",
       "      <td>case20_day0</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case20_day0_slice_0001</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case20</td>\n",
       "      <td>case20_day0</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case20_day0_slice_0001</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case20</td>\n",
       "      <td>case20_day0</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case20_day0_slice_0002</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case20</td>\n",
       "      <td>case20_day0</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case20_day0_slice_0002</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case20</td>\n",
       "      <td>case20_day0</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>case24_day25_slice_0143</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case24</td>\n",
       "      <td>case24_day25</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>case24_day25_slice_0143</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case24</td>\n",
       "      <td>case24_day25</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>case24_day25_slice_0144</td>\n",
       "      <td>large_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case24</td>\n",
       "      <td>case24_day25</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>case24_day25_slice_0144</td>\n",
       "      <td>small_bowel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case24</td>\n",
       "      <td>case24_day25</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>case24_day25_slice_0144</td>\n",
       "      <td>stomach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>case24</td>\n",
       "      <td>case24_day25</td>\n",
       "      <td>/Users/arahjou/Downloads/uw_madison/test/train...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3024 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id        class segmentation patient          days  \\\n",
       "0      case20_day0_slice_0001  large_bowel          NaN  case20   case20_day0   \n",
       "1      case20_day0_slice_0001  small_bowel          NaN  case20   case20_day0   \n",
       "2      case20_day0_slice_0001      stomach          NaN  case20   case20_day0   \n",
       "3      case20_day0_slice_0002  large_bowel          NaN  case20   case20_day0   \n",
       "4      case20_day0_slice_0002  small_bowel          NaN  case20   case20_day0   \n",
       "...                       ...          ...          ...     ...           ...   \n",
       "3019  case24_day25_slice_0143  small_bowel          NaN  case24  case24_day25   \n",
       "3020  case24_day25_slice_0143      stomach          NaN  case24  case24_day25   \n",
       "3021  case24_day25_slice_0144  large_bowel          NaN  case24  case24_day25   \n",
       "3022  case24_day25_slice_0144  small_bowel          NaN  case24  case24_day25   \n",
       "3023  case24_day25_slice_0144      stomach          NaN  case24  case24_day25   \n",
       "\n",
       "                                            image_files  spacing_x  spacing_y  \\\n",
       "0     /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "1     /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "2     /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "3     /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "4     /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "...                                                 ...        ...        ...   \n",
       "3019  /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "3020  /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "3021  /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "3022  /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "3023  /Users/arahjou/Downloads/uw_madison/test/train...        1.5        1.5   \n",
       "\n",
       "      size_x  size_y  slice  \n",
       "0        266     266    131  \n",
       "1        266     266    131  \n",
       "2        266     266    131  \n",
       "3        266     266     32  \n",
       "4        266     266     32  \n",
       "...      ...     ...    ...  \n",
       "3019     266     266     45  \n",
       "3020     266     266     45  \n",
       "3021     266     266    128  \n",
       "3022     266     266    128  \n",
       "3023     266     266    128  \n",
       "\n",
       "[3024 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_csv(\"/Users/arahjou/Downloads/uw_madison/test/train.csv\")\n",
    "df_train = df_train.sort_values([\"id\", \"class\"]).reset_index(drop = True)\n",
    "df_train[\"patient\"] = df_train.id.apply(lambda x: x.split(\"_\")[0])\n",
    "df_train[\"days\"] = df_train.id.apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "\n",
    "all_image_files = sorted(glob.glob(\"/Users/arahjou/Downloads/uw_madison/test/train/*/*/scans/*.png\"), key = lambda x: x.split(\"/\")[3] + \"_\" + x.split(\"/\")[5])\n",
    "size_x = [int(os.path.basename(_)[:-4].split(\"_\")[-4]) for _ in all_image_files]\n",
    "size_y = [int(os.path.basename(_)[:-4].split(\"_\")[-3]) for _ in all_image_files]\n",
    "spacing_x = [float(os.path.basename(_)[:-4].split(\"_\")[-2]) for _ in all_image_files]\n",
    "spacing_y = [float(os.path.basename(_)[:-4].split(\"_\")[-1]) for _ in all_image_files]\n",
    "df_train[\"image_files\"] = np.repeat(all_image_files, 3)\n",
    "df_train[\"spacing_x\"] = np.repeat(spacing_x, 3)\n",
    "df_train[\"spacing_y\"] = np.repeat(spacing_y, 3)\n",
    "df_train[\"size_x\"] = np.repeat(size_x, 3)\n",
    "df_train[\"size_y\"] = np.repeat(size_y, 3)\n",
    "df_train[\"slice\"] = np.repeat([int(os.path.basename(_)[:-4].split(\"_\")[-5]) for _ in all_image_files], 3)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Make mmseg-format data (2.5D by default)\n",
    "\n",
    "### Resizing\n",
    "### Creating Mask\n",
    "### Verifing the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "def rle_decode(mask_rle, shape):\n",
    "    s = np.array(mask_rle.split(), dtype=int)\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    h, w = shape\n",
    "    img = np.zeros((h * w,), dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo: hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# Ensure the output directories exist\n",
    "output_base_path = \"./mmseg_test\"\n",
    "os.makedirs(os.path.join(output_base_path, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, \"labels\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, \"splits\"), exist_ok=True)\n",
    "\n",
    "# Verify df_train is loaded properly\n",
    "if 'df_train' not in locals():\n",
    "    print(\"DataFrame 'df_train' is not defined.\")\n",
    "    # Load or define df_train here\n",
    "else:\n",
    "    for day, group in tqdm(df_train.groupby(\"days\")):\n",
    "        patient = group.patient.iloc[0]\n",
    "        imgs = []\n",
    "        msks = []\n",
    "        file_names = []\n",
    "        \n",
    "        for file_name in group.image_files.unique():\n",
    "            img = cv2.imread(file_name, cv2.IMREAD_ANYDEPTH)\n",
    "            if img is None:\n",
    "                print(f\"Failed to read image {file_name}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            segms = group.loc[group.image_files == file_name]\n",
    "            masks = {}\n",
    "            for segm, label in zip(segms.segmentation, segms[\"class\"]):\n",
    "                if not pd.isna(segm):\n",
    "                    mask = rle_decode(segm, img.shape[:2])\n",
    "                    masks[label] = mask\n",
    "                else:\n",
    "                    masks[label] = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "            masks = np.stack([masks[k] for k in sorted(masks)], -1)\n",
    "            imgs.append(img)\n",
    "            msks.append(masks)\n",
    "        \n",
    "        if imgs and msks:\n",
    "            imgs = np.stack(imgs, 0)\n",
    "            msks = np.stack(msks, 0)\n",
    "            for i in range(msks.shape[0]):\n",
    "                img = imgs[[max(0, i - 2), i, min(imgs.shape[0] - 1, i + 2)]].transpose(1, 2, 0)  # 2.5d data\n",
    "                msk = msks[i]\n",
    "                new_file_name = f\"{day}_{i}.png\"\n",
    "                if not cv2.imwrite(f\"{output_base_path}/images/{new_file_name}\", img):\n",
    "                    print(f\"Failed to write image file: {output_base_path}/images/{new_file_name}\")\n",
    "                if not cv2.imwrite(f\"{output_base_path}/labels/{new_file_name}\", msk):\n",
    "                    print(f\"Failed to write label file: {output_base_path}/labels/{new_file_name}\")\n",
    "        else:\n",
    "            print(f\"No images or masks found for day {day}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Make fold splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_files = glob.glob(\"./mmseg_train/images/*\")\n",
    "patients = [os.path.basename(_).split(\"_\")[0] for _ in all_image_files]\n",
    "\n",
    "\n",
    "split = list(GroupKFold(5).split(patients, groups = patients))\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(split):\n",
    "    with open(f\"./mmseg_train/splits/fold_{fold}.txt\", \"w\") as f:\n",
    "        for idx in train_idx:\n",
    "            f.write(os.path.basename(all_image_files[idx])[:-4] + \"\\n\")\n",
    "    with open(f\"./mmseg_train/splits/holdout_{fold}.txt\", \"w\") as f:\n",
    "        for idx in valid_idx:\n",
    "            f.write(os.path.basename(all_image_files[idx])[:-4] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augmentation=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = [img for img in os.listdir(image_dir) if img.endswith('.png')]\n",
    "        self.augmentation = augmentation\n",
    "        self.to_tensor = ToTensor()  # Converts numpy array (H x W x C) in the range [0, 255] to a torch.FloatTensor (C x H x W) in the range [0.0, 1.0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.augmentation:\n",
    "            augmented = self.augmentation(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        image = self.to_tensor(image)  # Ensure correct dimension order and scaling for model input\n",
    "        mask = torch.from_numpy(mask).long()  # Ensure mask is a long tensor\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Basic configurations\n",
    "num_classes = 3\n",
    "data_root = '/Users/arahjou/Downloads/uw_madison/mmseg_train'\n",
    "img_size = 256\n",
    "\n",
    "# Define the model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "\n",
    "# Define training and validation data paths\n",
    "train_img_dir = os.path.join(data_root, 'images')\n",
    "train_ann_dir = os.path.join(data_root, 'labels')\n",
    "val_img_dir = os.path.join(data_root, 'images')\n",
    "val_ann_dir = os.path.join(data_root, 'labels')\n",
    "\n",
    "# Define data transformations using albumentations\n",
    "train_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_img_dir, train_ann_dir, augmentation=train_transform)\n",
    "valid_dataset = CustomDataset(val_img_dir, val_ann_dir, augmentation=val_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "def train_one_epoch(epoch, model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Start training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch, model, train_loader, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "def tversky_loss(pred, target, alpha=0.5, beta=0.5, smooth=1.):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    TP = (pred * target).sum()\n",
    "    FP = ((1 - target) * pred).sum()\n",
    "    FN = (target * (1 - pred)).sum()\n",
    "\n",
    "    Tversky = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth)\n",
    "    return 1 - Tversky\n",
    "\n",
    "def focal_tversky_loss(pred, target, alpha=0.5, beta=0.5, gamma=1.33, smooth=1.):\n",
    "    tversky_index = tversky_loss(pred, target, alpha, beta, smooth)\n",
    "    focal_tversky = torch.pow((1 - tversky_index), gamma)\n",
    "    return focal_tversky\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified 3.4.1 code to include metrics and visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.11166023750907726, Valid Loss: 0.004622474981092468\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from albumentations import Compose, Resize, Normalize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augmentation=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = [img for img in os.listdir(image_dir) if img.endswith('.png')]\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx].replace('image', 'mask'))  # Assuming mask filenames are predictable\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.augmentation:\n",
    "            augmented = self.augmentation(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask'].long()\n",
    "\n",
    "        #mask = torch.from_numpy(mask).long()\n",
    "        return image, mask\n",
    "\n",
    "# Configuration settings\n",
    "num_classes = 3\n",
    "data_root = '/Users/arahjou/Downloads/uw_madison/'\n",
    "img_size = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Path configurations\n",
    "train_img_dir = os.path.join(data_root, 'mmseg_train/images')\n",
    "train_ann_dir = os.path.join(data_root, 'mmseg_train/labels')\n",
    "val_img_dir = os.path.join(data_root, 'mmseg_val/images')\n",
    "val_ann_dir = os.path.join(data_root, 'mmseg_val/labels')\n",
    "test_img_dir = os.path.join(data_root, 'mmseg_test/images')\n",
    "test_ann_dir = os.path.join(data_root, 'mmseg_test/labels')\n",
    "\n",
    "# Define data transformations\n",
    "train_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = CustomDataset(train_img_dir, train_ann_dir, augmentation=train_transform)\n",
    "valid_dataset = CustomDataset(val_img_dir, val_ann_dir, augmentation=val_transform)\n",
    "test_dataset = CustomDataset(test_img_dir, test_ann_dir, augmentation=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training and validation function\n",
    "def train_and_validate(model, train_loader, valid_loader, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss / len(train_loader)}, Valid Loss: {valid_loss / len(valid_loader)}\")\n",
    "\n",
    "# Evaluate function for testing\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader)}\")\n",
    "\n",
    "# Execute training and validation\n",
    "train_and_validate(model, train_loader, valid_loader, optimizer, scheduler, num_epochs=10)\n",
    "\n",
    "# Execute testing\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/Users/arahjou/Downloads/uw_madison/mmseg_train/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Handeling gray image and RGB masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, augmentation=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = [img for img in os.listdir(image_dir) if img.endswith('.png')]\n",
    "        self.augmentation = augmentation\n",
    "        self.to_tensor = ToTensor()  # Converts numpy array (H x W x C) in the range [0, 255] to a torch.FloatTensor (C x H x W) in the range [0.0, 1.0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "\n",
    "        # Load image in grayscale and expand to three channels\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Load mask in RGB\n",
    "        mask = cv2.imread(mask_path)\n",
    "\n",
    "        if self.augmentation:\n",
    "            augmented = self.augmentation(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        image = self.to_tensor(image)  # Ensure correct dimension order and scaling for model input\n",
    "        mask = torch.from_numpy(mask[:, :, 0]).long()  # Convert mask to long tensor, assuming mask is single-channel relevant info in red channel\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Basic configurations\n",
    "num_classes = 3\n",
    "data_root = '/Users/arahjou/Downloads/uw_madison/mmseg_train'\n",
    "img_size = 256\n",
    "\n",
    "# Define the model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "\n",
    "# Define training and validation data paths\n",
    "train_img_dir = os.path.join(data_root, 'images')\n",
    "train_ann_dir = os.path.join(data_root, 'labels')\n",
    "val_img_dir = os.path.join(data_root, 'images')\n",
    "val_ann_dir = os.path.join(data_root, 'labels')\n",
    "\n",
    "# Define data transformations using albumentations\n",
    "train_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(img_size, img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0)\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_img_dir, train_ann_dir, augmentation=train_transform)\n",
    "valid_dataset = CustomDataset(val_img_dir, val_ann_dir, augmentation=val_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "def train_one_epoch(epoch, model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Start training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch, model, train_loader, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 saving model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the entire model\n",
    "torch.save(model, '/Users/arahjou/Downloads/uw_madison/mmseg_train/model/model.pth')\n",
    "\n",
    "# Loading the entire model\n",
    "model = torch.load('/Users/arahjou/Downloads/uw_madison/mmseg_train/model/model.pth')\n",
    "model.eval()\n",
    "\n",
    "# Saving the state dictionary\n",
    "torch.save(model.state_dict(), '/Users/arahjou/Downloads/uw_madison/mmseg_train/model/model_state_dict.pth')\n",
    "num_classes = 3\n",
    "# Loading the state dictionary\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=None,  # Set to None to not load default pretrained weights\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "model.load_state_dict(torch.load('/Users/arahjou/Downloads/uw_madison/mmseg_train/model/model_state_dict.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 (saving each cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}, Loss: {average_loss}\")\n",
    "    return average_loss\n",
    "\n",
    "# Start training and save the model at the end of each epoch\n",
    "num_epochs = 10\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_one_epoch(epoch, model, train_loader, optimizer)\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), f'/Users/arahjou/Downloads/uw_madison/model/best_model_epoch_{epoch}.pth')\n",
    "        print(f\"Saved Best Model at Epoch {epoch} with Loss {best_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Using Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Your Pre-trained Model\n",
    "num_classes = 3\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",\n",
    "    encoder_weights=None,  # Assuming you are loading your custom trained weights\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ")\n",
    "model.load_state_dict(torch.load('/Users/arahjou/Downloads/uw_madison/model/model_state_dict.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the Image\n",
    "# Define the image size\n",
    "img_size = 266  # Assuming the size of the image you want\n",
    "\n",
    "# Define the transformation using only albumentations\n",
    "transform = Compose([\n",
    "    Resize(height=img_size, width=img_size),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_and_transform_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"No image found at {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transformed = transform(image=image)  # Apply the transformations\n",
    "    image_tensor = transformed['image'].unsqueeze(0)  # Add batch dimension\n",
    "    return image_tensor\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/arahjou/Downloads/uw_madison/train/case9/case9_day22/scans/slice_0071_360_310_1.50_1.50.png'\n",
    "image_tensor = load_and_transform_image(image_path)\n",
    "print(image_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Perform Inference\n",
    "with torch.no_grad():  # Turn off gradients to speed up this part\n",
    "    output = model(image_tensor)\n",
    "    prediction = torch.argmax(output, dim=1)  # Get the most likely class for each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Post-process the Output\n",
    "predicted_mask = prediction.squeeze().cpu().numpy()  # Remove batch dimension and convert to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize the Results\n",
    "\n",
    "plt.imshow(predicted_mask, cmap='gray')  # Assuming the mask is grayscale\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".C_Conda",
   "language": "python",
   "name": ".c_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
