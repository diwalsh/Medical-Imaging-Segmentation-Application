{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09ea206-e2f7-4bc6-8f50-9982911abcdb",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "479f43d6-3117-4375-bf93-cb18a57f0045",
   "metadata": {},
   "source": [
    "The aim of this notebook is to evaluate the proposed modeling pipeline in terms of segmentation precision (quality) and segmentation time (performance). The models under evaluation are listed next:\n",
    "1. **SegFormer1 (baseline)**: This model is based on Transformers and has been taken from https://learnopencv.com/medical-image-segmentation/. The model has been trained using only segmented images. SegFormer1 is our baseline for comparison purposes.\n",
    "2. **ResNetClassifier1 + SegFormer1**: In this approach, a binary classifier is employed prior to segmentation. This classifier determines whether a given slice contains any of the organs of interest —namely, the stomach, small bowel, or large bowel— assigning a label of \"1\" if present, or \"0\" if absent. The classifier uses the pre-trained convolutional layers of the ResNet50V2 network architecture, with two additional task-specific layers appended at the end: a fully connected layer with 256 neurons, followed by an output sigmoid layer. If the classifier identifies the presence of any of the organs of interest in the input image, the segmentation module is subsequently activated.\n",
    "\n",
    "<img src=\"images/model_workflow.png\" width=\"1000\">\n",
    "\n",
    "3. **ResNetClassifier2 + SegFormer1**: Similar to model #2. The difference is that the classifier has been trained with a bigger batch size.\n",
    "4. **SegFormer2**: This model shares the same architecture as SegFormer1. SegFormer2 has been trained using both segmented and non-segmented slices.\n",
    "5. **ResNetClassifier1 + SegFormer2**: Similar to model #2, except that the segmentation model is SegFormer2.\n",
    "6. **ResNetClassifier2 + SegFormer2**: Similar to model #3, except that the segmentation model is SegFormer2.\n",
    "\n",
    "The evaluation metrics are:\n",
    "* **Dice coefficient**: It measures the spatial overlap between two segmentations. It is given by:\n",
    "$$\n",
    "DC = \\frac{2 \\times |A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "Where A and B are the masks associated with the ground truth and prediction, respectively. For multiclass segmentation, the coefficient is computed separately for each class, and then the results are averaged.\n",
    "* **Segmentation time**: The segmentation time refers to the duration, in seconds, from when an image is inputted into the model to when the corresponding mask is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d26e78-b19d-4d83-89f3-24abc11e1cbf",
   "metadata": {},
   "source": [
    "# Authors\n",
    "Tina Mangum, Ali Rahjouei, Sergio Sanz, Di Walsh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b203b-9f44-4fd9-a83f-1151e8526aad",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001c1515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:38.775081Z",
     "start_time": "2023-07-14T07:54:27.162224Z"
    },
    "executionInfo": {
     "elapsed": 6564,
     "status": "ok",
     "timestamp": 1713964814800,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "001c1515"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssre_\\.venv_common\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import platform\n",
    "import warnings\n",
    "import time\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# To filter UserWarning.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For data augmentation and preprocessing\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Imports required SegFormer classes\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# Importing lighting along with a built-in callback it provides.\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Importing torchmetrics modular and functional implementations.\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# To print model summary.\n",
    "#from torchinfo import summary\n",
    "\n",
    "# Tensor and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880832c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T07:54:38.807356Z",
     "start_time": "2023-07-14T07:54:38.775081Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713964814801,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "880832c0"
   },
   "outputs": [],
   "source": [
    "# Sets the internal precision of float32 matrix multiplications.\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# To enable determinism.\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "# To render the matplotlib figure in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc3a642-e4df-4d08-b079-db9e134ca051",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = \"2d_all\"\n",
    "MODEL_PATH = \"models\"\n",
    "CLASS_MODEL1 = \"resnet50v2_nn256_lr0001_relu_batch32_epoch30_v2.keras\"\n",
    "CLASS_MODEL2 = \"resnet50v2_nn256_lr0001_relu_batch64_epoch30_v4.keras\"\n",
    "SEG_MODEL_SLC = \"ckpt_049-vloss_0.3115_vf1_0.9401_batch16.ckpt\"\n",
    "SEG_MODEL_ALL = \"ckpt_049-vloss_0.5816_vf1_0.8927_batch12.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843bec2-2fd5-49e8-a738-421ac54b6aee",
   "metadata": {},
   "source": [
    "# Configuration Class Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b139322-e1a6-488c-b090-48c59b8f21d8",
   "metadata": {},
   "source": [
    "Let's define first some configuration classes including image-related and preprocessing parameters, and model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9191d6a-27dd-45f6-a04c-74181ab56315",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1713965426466,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "de70d1ed",
    "outputId": "17bab34d-e3f2-4cc2-8fca-71273e7671d3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A class that holds all the hyperparameters we will use to process images.\n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    NUM_CLASSES:   int = 4 # including background.\n",
    "    IMAGE_SIZE: tuple[int,int] = (288, 288) # W, H\n",
    "    MEAN: tuple = (0.485, 0.456, 0.406)\n",
    "    STD:  tuple = (0.229, 0.224, 0.225)\n",
    "    MEAN_CLF: float = 0.136\n",
    "    STD_CLF: float = 0.178\n",
    "    BACKGROUND_CLS_ID: int = 0\n",
    "    URL: str = r\"https://www.dropbox.com/scl/fi/r0685arupp33sy31qhros/dataset_UWM_GI_Tract_train_valid.zip?rlkey=w4ga9ysfiuz8vqbbywk0rdnjw&dl=1\"\n",
    "    DATASET_PATH: str = os.path.join(os.getcwd(), ROOT_PATH)\n",
    "    MODEL_NAME_CLF1 = os.path.join(MODEL_PATH, CLASS_MODEL1)\n",
    "    MODEL_NAME_CLF2 = os.path.join(MODEL_PATH, CLASS_MODEL2)\n",
    "    MODEL_NAME_SEG_SLC = os.path.join(MODEL_PATH, SEG_MODEL_SLC)\n",
    "    MODEL_NAME_SEG_ALL = os.path.join(MODEL_PATH, SEG_MODEL_ALL)\n",
    "    THR1: float = 0.019938506186008453\n",
    "    THR2: float = 0.07652711868286133\n",
    "\n",
    "# A class containing the locations of the images and masks of these model evaluation tests\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    DATA_VALID_IMAGES: str = os.path.join(DatasetConfig.DATASET_PATH, \"valid\", \"images\", r\"*.png\")\n",
    "    DATA_VALID_LABELS: str = os.path.join(DatasetConfig.DATASET_PATH, \"valid\", \"masks\",  r\"*.png\")\n",
    "\n",
    "# A class that holds all the hyperparameters for training and evaluation.\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:      int = 12 # 8\n",
    "    NUM_EPOCHS:      int = 1\n",
    "    INIT_LR:       float = 3e-4\n",
    "    NUM_WORKERS:     int = 0 if platform.system() == \"Windows\" else os.cpu_count()\n",
    "\n",
    "    OPTIMIZER_NAME:  str = \"AdamW\"\n",
    "    WEIGHT_DECAY:  float = 1e-4\n",
    "    USE_SCHEDULER:  bool = True # Use learning rate scheduler?\n",
    "    SCHEDULER:       str = \"MultiStepLR\" # Name of the scheduler to use.\n",
    "    MODEL_NAME:str = \"nvidia/segformer-b4-finetuned-ade-512-512\"\n",
    "\n",
    "# A class that contains the (optional) batch size and the number of batches to be used to display for inference results.\n",
    "#@dataclass\n",
    "class InferenceConfig:\n",
    "    BATCH_SIZE:  int = 10\n",
    "    NUM_BATCHES: int = 2\n",
    "\n",
    "DatasetConfig.NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd876f-0761-4b09-a009-001034e524d1",
   "metadata": {},
   "source": [
    "# Class Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38fece-1885-4bdd-a2f1-f539ddeee098",
   "metadata": {},
   "source": [
    "This section describes a custom class called MedicalDataset for creating training and validation (segmentation) dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e76e6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.876Z"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1713965429746,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "11e76e6b"
   },
   "outputs": [],
   "source": [
    "# Class inherited from Pytorch Dataset\n",
    "class MedicalDataset(Dataset):\n",
    "    # Initialization function\n",
    "    def __init__(self, *, image_paths, mask_paths, img_size, ds_mean, ds_std, ds_mean_clf, ds_std_clf):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths  = mask_paths\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean = ds_mean\n",
    "        self.ds_std = ds_std\n",
    "        self.ds_mean_clf = ds_mean_clf\n",
    "        self.ds_std_clf = ds_std_clf\n",
    "        self.transforms  = self.setup_transforms(mean=self.ds_mean, std=self.ds_std)\n",
    "\n",
    "    # Calculates the number of images used for training or validation\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # Normalization Function for binary classification (stomach/small/large bowel: yes, no)\n",
    "    def normalize_classif(self, image):\n",
    "        image = image / 255.0\n",
    "        image = (image - self.ds_mean_clf) / self.ds_std_clf\n",
    "        return image\n",
    "\n",
    "    # Preprocess transforms - Normalization and converting to PyTorch tensor format (HWC --> CHW).\n",
    "    def setup_transforms(self, *, mean, std):\n",
    "        transforms = []        \n",
    "        transforms.extend([\n",
    "                A.Normalize(mean=mean, std=std, always_apply=True),\n",
    "                ToTensorV2(always_apply=True),  # (H, W, C) --> (C, H, W)\n",
    "        ])\n",
    "        return A.Compose(transforms)\n",
    "\n",
    "    # load and resize the image to the specified size.\n",
    "    # The interpolation method used is nearest-neighbor for the segmentation model.\n",
    "    def load_file_nearest(self, file_path, depth=0):\n",
    "        file = cv2.imread(file_path, depth)\n",
    "        if depth == cv2.IMREAD_COLOR:\n",
    "            file = file[:, :, ::-1]\n",
    "        return cv2.resize(file, (self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # load and resize the image to the specified size.\n",
    "    # The interpolation method used is linear for the classification model.\n",
    "    def load_file_linear(self, file_path, depth=0):\n",
    "        file = cv2.imread(file_path, depth)\n",
    "        if depth == cv2.IMREAD_COLOR:\n",
    "            file = file[:, :, ::-1]\n",
    "        return cv2.resize(file, (self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load and preprocess image for classification\n",
    "        image_clf = self.load_file_linear(self.image_paths[index], depth=cv2.IMREAD_COLOR)\n",
    "        image_clf = self.normalize_classif(image_clf)\n",
    "        \n",
    "        # Load and preprocess image and mask file, both for segmentation\n",
    "        image = self.load_file_nearest(self.image_paths[index], depth=cv2.IMREAD_COLOR)       \n",
    "        mask  = self.load_file_nearest(self.mask_paths[index],  depth=cv2.IMREAD_GRAYSCALE)        \n",
    "        transformed = self.transforms(image=image, mask=mask)\n",
    "\n",
    "        # Return the loeaded images\n",
    "        image, mask = transformed[\"image\"], transformed[\"mask\"].to(torch.long)        \n",
    "        return image_clf, image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9654351-032e-487a-b3f2-3fb5dfff092f",
   "metadata": {},
   "source": [
    "**Class MedicalSegmentationDataModule**: It helps organize and encapsulate all the data-related operations and logic in a PyTorch project. It acts as a bridge between your data and Lightning’s training pipeline. It is a convenient abstraction that encapsulates data-related operations, promotes code organization, and facilitates seamless integration with other Lightning components for efficient and reproducible deep-learning experiments.\n",
    "\n",
    "**LightningDataModule**: A data module that encapsulates the five steps involved in data processing in PyTorch: Download / tokenize / process; Clean and (maybe) save to disk; Load inside Dataset; Apply transforms (rotate, tokenize, etc…); Wrap inside a DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc1d17b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-14T07:54:25.880Z"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1713965435112,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "3dc1d17b"
   },
   "outputs": [],
   "source": [
    "# Define MedicalSegmentationDataModule\n",
    "class MedicalSegmentationDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        img_size=(384, 384),\n",
    "        ds_mean=(0.485, 0.456, 0.406),\n",
    "        ds_std=(0.229, 0.224, 0.225),\n",
    "        ds_mean_clf=0.136,\n",
    "        ds_std_clf=0.178,\n",
    "        batch_size=12,\n",
    "        num_workers=3,\n",
    "        pin_memory=False,\n",
    "        shuffle_validation=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size    = img_size\n",
    "        self.ds_mean     = ds_mean\n",
    "        self.ds_std      = ds_std\n",
    "        self.ds_mean_clf = ds_mean_clf\n",
    "        self.ds_std_clf  = ds_std_clf\n",
    "        self.batch_size  = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory  = pin_memory\n",
    "        self.shuffle_validation = shuffle_validation\n",
    "\n",
    "    # Create validation dataset and dataloader.\n",
    "    def setup(self, *args, **kwargs):        \n",
    "        valid_imgs = sorted(glob(f\"{Paths.DATA_VALID_IMAGES}\"))        \n",
    "        valid_msks = sorted(glob(f\"{Paths.DATA_VALID_LABELS}\"))\n",
    "        self.valid_ds = MedicalDataset(image_paths=valid_imgs, mask_paths=valid_msks, img_size=self.img_size, \n",
    "                                       ds_mean=self.ds_mean, ds_std=self.ds_std,\n",
    "                                       ds_mean_clf=self.ds_mean_clf, ds_std_clf=self.ds_std_clf)\n",
    "\n",
    "    # Create validation dataloader object.\n",
    "    def val_dataloader(self):        \n",
    "        return DataLoader(\n",
    "            self.valid_ds, batch_size=self.batch_size,  pin_memory=self.pin_memory,\n",
    "            num_workers=self.num_workers, shuffle=self.shuffle_validation\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97497ce-a775-41a9-9cc3-da4fe5669878",
   "metadata": {},
   "source": [
    "**Class MedicalSegmentationModel**: This class describes the variables, hyperparameters, and functions associated with the SegFormer segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec06e142",
   "metadata": {
    "editable": true,
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1713965490343,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "ec06e142",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading a pre-trained model version\n",
    "def get_model(*, model_name, num_classes):\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "class MedicalSegmentationModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_classes: int = 10,\n",
    "        init_lr: float = 0.001,\n",
    "        optimizer_name: str = \"Adam\",\n",
    "        weight_decay: float = 1e-4,\n",
    "        use_scheduler: bool = False,\n",
    "        scheduler_name: str = \"multistep_lr\",\n",
    "        num_epochs: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save the arguments as hyperparameters.\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Loading model using the function defined above.\n",
    "        self.model = get_model(model_name=self.hparams.model_name, num_classes=self.hparams.num_classes)\n",
    "\n",
    "        # Initializing the required metric objects.\n",
    "        self.mean_train_loss = MeanMetric()\n",
    "        self.mean_train_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "        self.mean_valid_loss = MeanMetric()\n",
    "        self.mean_valid_f1 = MulticlassF1Score(num_classes=self.hparams.num_classes, average=\"macro\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        outputs = self.model(pixel_values=data, return_dict=True)\n",
    "        upsampled_logits = F.interpolate(outputs[\"logits\"], size=data.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return upsampled_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d94d5-9ee8-42fa-9f64-ecb276ab74c0",
   "metadata": {},
   "source": [
    "This section comprises three functions:\n",
    "* **Dice Coefficient Function**: Computes the Dice coefficient, a measure of spatial overlap, used for evaluating segmentation accuracy.\n",
    "* **Mask Detection Function**: Detects masks containing any of the segmented organs, a prerequisite for Dice coefficient evaluation.\n",
    "* **Prediction and Metric Computation Function (Inference)**: Performs inference to make predictions and computes evaluation metrics based on the ground truth and predicted masks.\n",
    "* **Metrics Printing Function**: Prints out the computed evaluation metrics for analysis and interpretation.\"Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60294fe7-c252-4c13-81bf-4ed210306cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dice coefficient\n",
    "def dice_coeff(predictions, ground_truths, num_classes=4, dims=(1, 2), smooth=1e-8):\n",
    "    # (batch_size, height, width, num_classes)\n",
    "    ground_truth_oh = F.one_hot(ground_truths, num_classes=num_classes)\n",
    "    # (batch_size, height, width, num_classes)\n",
    "    prediction_norm = F.one_hot(predictions, num_classes=num_classes)\n",
    "    # (batch_size, num_classes)\n",
    "    intersection = (prediction_norm * ground_truth_oh).sum(dim=dims)\n",
    "    # (batch_size, num_classes)\n",
    "    summation = prediction_norm.sum(dim=dims) + ground_truth_oh.sum(dim=dims)\n",
    "    # (batch_size, num_classes)\n",
    "    dice = (2.0 * intersection + smooth) / (summation + smooth)\n",
    "    # Compute averages: overall and per class\n",
    "    dice_mean = dice.mean()\n",
    "    dice_mean_per_class = dice.mean(dim=0)\n",
    "    return np.concatenate([np.array([dice_mean]), dice_mean_per_class])\n",
    "\n",
    "# Detect if the mask includes any of the classes\n",
    "def detect_nonzero_masks(masks):\n",
    "    # Check if any element is nonzero along the last two axes\n",
    "    nonzero_masks_mask = np.any(masks != 0, axis=(1, 2))\n",
    "    # Get the indices of the zero images\n",
    "    nonzero_masks_indices = np.where(nonzero_masks_mask)[0]\n",
    "    return nonzero_masks_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e243e55-73e3-4ad3-a3a9-0f669fa8f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that performs the evaluation process of the model\n",
    "@torch.inference_mode()\n",
    "def inference(class_model, seg_model, loader, img_size, device=\"cpu\", enable_clf=False, threshold_clf=0):\n",
    "    \n",
    "    cont = 0\n",
    "    cont_2 = 0\n",
    "    score_sum = 0\n",
    "    score_sum_2 = 0\n",
    "    score_sum_per_class = np.zeros(4)\n",
    "    score_sum_per_class_2 = np.zeros(4)\n",
    "    score = 1.0\n",
    "    score_2 = 1.0\n",
    "    score_ave = 1.0\n",
    "    score_ave_2 = 1.0\n",
    "    score_ave_per_class = np.ones(4)\n",
    "    score_ave_per_class_2 = np.ones(4)\n",
    "    time_sum = 0\n",
    "\n",
    "    for idx, (batch_img_clf, batch_img_seg, batch_mask) in enumerate(loader):\n",
    "\n",
    "        batch_size = batch_mask.shape[0]\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        if enable_clf: # Classification -> Segmentation\n",
    "        \n",
    "            # Classification predictions\n",
    "            y_pred_clf = class_model.predict(batch_img_clf, verbose=0).reshape(-1)\n",
    "            clf_labels = y_pred_clf > threshold_clf\n",
    "            true_idxs = np.where(clf_labels == True)[0]\n",
    "            \n",
    "            # Segmentation predictions            \n",
    "            pred_all = torch.Tensor(np.zeros(batch_mask.shape)).long()\n",
    "            if len(true_idxs) > 0:\n",
    "                predictions = seg_model(batch_img_seg[true_idxs].to(device))\n",
    "                pred_all[true_idxs] = torch.from_numpy(predictions.argmax(dim=1).cpu().numpy())\n",
    "                pred_all = torch.tensor(pred_all)\n",
    "                \n",
    "        else: # Segmentation only\n",
    "            \n",
    "            # Segmentation predictions\n",
    "            predictions = seg_model(batch_img_seg.to(device))\n",
    "            pred_all = predictions.argmax(dim=1).cpu().numpy()\n",
    "            pred_all = torch.tensor(pred_all)\n",
    "\n",
    "        # Compute modeling time\n",
    "        end = time.perf_counter()\n",
    "        time_sum = time_sum + ((end - start) / batch_size)\n",
    "\n",
    "        # Compute Dice coefficient\n",
    "        cont = cont + 1\n",
    "        score = dice_coeff(pred_all, batch_mask)\n",
    "        score_sum = score_sum + score[0]        \n",
    "        score_sum_per_class = score_sum_per_class + score[1:]        \n",
    "        \n",
    "        nonzero_mask_idxs = detect_nonzero_masks(batch_mask.numpy())\n",
    "        \n",
    "        if len(nonzero_mask_idxs) > 0:\n",
    "            cont_2 = cont_2 + 1\n",
    "            score_2 = dice_coeff(pred_all[nonzero_mask_idxs],batch_mask[nonzero_mask_idxs])\n",
    "            score_sum_2 = score_sum_2 + score_2[0]            \n",
    "            score_sum_per_class_2 = score_sum_per_class_2 + score_2[1:]            \n",
    "\n",
    "        #if idx == InferenceConfig.NUM_BATCHES * 2:\n",
    "        #    break\n",
    "\n",
    "    score_ave = score_sum / cont\n",
    "    score_ave_2 = score_sum_2 / cont_2\n",
    "    score_ave_per_class = score_sum_per_class / cont\n",
    "    score_ave_per_class_2 = score_sum_per_class_2 / cont_2\n",
    "    time_per_image = time_sum / cont\n",
    "\n",
    "    return score_ave, score_ave_2, score_ave_per_class, score_ave_per_class_2, time_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53c7820e-50b6-4dbb-a05d-7dea3d65c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report function\n",
    "def print_report(score_ave_per_class, score_ave_per_class_2, time_per_image, decimals=5):\n",
    "    print(\"-----\" * 7)\n",
    "    print(f\"ave dice_score (all masks): {np.round(np.mean(score_ave_per_class[1:]),decimals)}\")\n",
    "    print(f\"ave dice_score (only segmented): {np.round(np.mean(score_ave_per_class_2[1:]),decimals)}\") \n",
    "    print(f\"ave dice_score per class (all masks): {np.round(score_ave_per_class,decimals)}\")\n",
    "    print(f\"ave dice_score per class (only segmented): {np.round(score_ave_per_class_2,decimals)}\") \n",
    "    print(f\"seconds per image: {np.round(time_per_image,decimals)}\")\n",
    "    print(f\"images per second: {np.round(1 / time_per_image,decimals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0a5b0-2312-4b97-bc17-e89530e4e631",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac90e2b-328b-4198-9283-cb41c9c0740d",
   "metadata": {},
   "source": [
    "This section is dedicated to testing and evaluating the six proposed models. The stored models are loaded, and subsequently, the inference function is called for each model. This process allows for the evaluation of each model's performance on the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf9155-4ca4-4246-9443-e54e2f9b479b",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24dacc05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1713965496076,
     "user": {
      "displayName": "Ali Rahjouei",
      "userId": "16342226973724525946"
     },
     "user_tz": -120
    },
    "id": "24dacc05",
    "outputId": "9c6eee87-3eb5-405a-d720-668ea35448e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Seed everything for reproducibility.\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# Get the validation dataloader.\n",
    "data_module = MedicalSegmentationDataModule(\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    img_size=DatasetConfig.IMAGE_SIZE,\n",
    "    ds_mean=DatasetConfig.MEAN,\n",
    "    ds_std=DatasetConfig.STD,\n",
    "    ds_mean_clf=DatasetConfig.MEAN_CLF,\n",
    "    ds_std_clf=DatasetConfig.STD_CLF,\n",
    "    batch_size=InferenceConfig.BATCH_SIZE,\n",
    "    num_workers=TrainingConfig.NUM_WORKERS,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "data_module.setup()\n",
    "valid_loader = data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef5fb7b6-33ad-4a3f-876b-4c01aa0a57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification model\n",
    "class_model_1 = load_model(DatasetConfig.MODEL_NAME_CLF1)\n",
    "class_model_2 = load_model(DatasetConfig.MODEL_NAME_CLF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a5a5d1e-f228-4544-a9a6-0aa2d0b4ec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([4, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b4-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([4, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load segmentation model (SegFormer 1)\n",
    "seg_model_1 = MedicalSegmentationModel.load_from_checkpoint(DatasetConfig.MODEL_NAME_SEG_SLC)\n",
    "seg_model_2 = MedicalSegmentationModel.load_from_checkpoint(DatasetConfig.MODEL_NAME_SEG_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4472f-098a-40f1-87f1-56f52e8cb32f",
   "metadata": {},
   "source": [
    "## SegFormer 1 (Trained With Only Segmented Slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac4d2856-7052-4379-b4c9-9c7f30d50076",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m seg_model_1\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m----> 5\u001b[0m _, _, score_ave_per_class_1, score_ave_per_class_2_1, time_per_image_1 \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_model_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_model_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDatasetConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_clf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_clf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m print_report(score_ave_per_class_1, score_ave_per_class_2_1, time_per_image_1)\n",
      "File \u001b[1;32m~\\.venv_common\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 66\u001b[0m, in \u001b[0;36minference\u001b[1;34m(class_model, seg_model, loader, img_size, device, enable_clf, threshold_clf)\u001b[0m\n\u001b[0;32m     61\u001b[0m         score_sum_per_class_2 \u001b[38;5;241m=\u001b[39m score_sum_per_class_2 \u001b[38;5;241m+\u001b[39m score_2[\u001b[38;5;241m1\u001b[39m:]            \n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m#if idx == InferenceConfig.NUM_BATCHES * 2:\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m#    break\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m score_ave \u001b[38;5;241m=\u001b[39m \u001b[43mscore_sum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcont\u001b[49m\n\u001b[0;32m     67\u001b[0m score_ave_2 \u001b[38;5;241m=\u001b[39m score_sum_2 \u001b[38;5;241m/\u001b[39m cont_2\n\u001b[0;32m     68\u001b[0m score_ave_per_class \u001b[38;5;241m=\u001b[39m score_sum_per_class \u001b[38;5;241m/\u001b[39m cont\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model_1.to(DEVICE)\n",
    "\n",
    "_, _, score_ave_per_class_1, score_ave_per_class_2_1, time_per_image_1 = inference(class_model_1, seg_model_1, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=False, threshold_clf=0)\n",
    "\n",
    "print_report(score_ave_per_class_1, score_ave_per_class_2_1, time_per_image_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43337381-dcbc-45fe-99ea-9bf0b063257d",
   "metadata": {},
   "source": [
    "## Classifier 1 + SegFormer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea86d2-effc-43cb-88aa-dc4c69d10304",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model_1.to(DEVICE)\n",
    "\n",
    "_, _, score_ave_per_class_11, score_ave_per_class_2_11, time_per_image_11 = inference(class_model_1, seg_model_1, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=True, threshold_clf=DatasetConfig.THR1)\n",
    "\n",
    "print_report(score_ave_per_class_11, score_ave_per_class_2_11, time_per_image_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487479fa-a24f-4b76-a431-3e871971e454",
   "metadata": {},
   "source": [
    "## Classifier 2 + SegFormer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8daa64-5f9b-41e9-896c-6daba8588a78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model_1.to(DEVICE)\n",
    "\n",
    "_, _, score_ave_per_class_21, score_ave_per_class_2_21, time_per_image_21 = inference(class_model_2, seg_model_1, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=True, threshold_clf=DatasetConfig.THR2)\n",
    "\n",
    "print_report(score_ave_per_class_21, score_ave_per_class_2_21, time_per_image_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5929dfa-a3b6-49b5-b0fd-377ee6c572f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## SegFormer 2 (Trained With All Slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45cd45-0bfe-47c0-8523-d40e98c86a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model_2.to(DEVICE)\n",
    "\n",
    "_, _, score_ave_per_class_2, score_ave_per_class_2_2, time_per_image_2 = inference(class_model_1, seg_model_2, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=False, threshold_clf=0)\n",
    "\n",
    "print_report(score_ave_per_class_2, score_ave_per_class_2_2, time_per_image_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf99085-9f25-4f9d-b1fb-cd4e2980a369",
   "metadata": {},
   "source": [
    "## Classifier 1 + Segformer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bcf56-9df3-4ef0-9215-1ce0a7e04aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model_2.to(DEVICE)\n",
    "\n",
    "_, _, score_ave_per_class_12, score_ave_per_class_2_12, time_per_image_12 = inference(class_model_1, seg_model_2, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=True, threshold_clf=DatasetConfig.THR1)\n",
    "\n",
    "print_report(score_ave_per_class_12, score_ave_per_class_2_12, time_per_image_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3bee8-6564-4e14-9b1f-a3165811b184",
   "metadata": {},
   "source": [
    "## Classifier 2 + Segformer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6566b34-bcf8-4979-bd41-cb83fc9b38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available.\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seg_model_2.to(DEVICE)\n",
    "\n",
    "_, _, score_ave_per_class_22, score_ave_per_class_2_22, time_per_image_22 = inference(class_model_2, seg_model_2, valid_loader, device=DEVICE, img_size=DatasetConfig.IMAGE_SIZE, enable_clf=True, threshold_clf=DatasetConfig.THR2)\n",
    "\n",
    "print_report(score_ave_per_class_22, score_ave_per_class_2_22, time_per_image_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30592eb6-9317-450b-9a79-3635a1e5e723",
   "metadata": {},
   "source": [
    "## Results per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182575c7-393d-42d9-9615-1471aae58c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "organs = (\"Stomach\", \"Small Bowel\", \"Large Bowel\")\n",
    "method = {\n",
    "    'SegFormer1': (score_ave_per_class_1[1], score_ave_per_class_1[2], score_ave_per_class_1[3]),\n",
    "    'Classif2 + SegFormer1': (score_ave_per_class_21[1], score_ave_per_class_21[2], score_ave_per_class_21[3]),\n",
    "    'SegFormer2': (score_ave_per_class_2[1], score_ave_per_class_2[2], score_ave_per_class_2[3]),\n",
    "    'Classif2 + SegFormer2': (score_ave_per_class_22[1], score_ave_per_class_22[2], score_ave_per_class_22[3]) \n",
    "}\n",
    "\n",
    "x = np.arange(len(organs))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "# Custom color palette\n",
    "#colors = ['#8B4513', '#CD853F', '#D2B48C', '#556B2F']\n",
    "colors = ['#D0D3D4', '#A6ACAF', '#7B8788', '#566573']\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained', figsize=[9,6])\n",
    "\n",
    "for attribute, measurement, color in zip(method.keys(), method.values(), colors):\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, np.round(measurement,2), width, label=attribute, color=color)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Dice Coefficient')\n",
    "ax.set_title('Dice Coefficient per Segment')\n",
    "ax.set_xticks(x + width, organs)\n",
    "ax.legend(loc='upper center', ncol=4)\n",
    "ax.set_ylim(0.5, 1)\n",
    "plt.savefig('images/dice_coeff.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60069303-3674-4a43-bb65-582ffa4b2e5c",
   "metadata": {},
   "source": [
    "The figure above shows the Dice coefficient per class for four of the evaluated algorithms. As can be seen, when adding the classification model to the workflow, the resulting coefficients are similar to each other, which means that the model performs well for each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335a045-e389-4466-9edf-7b34220d5227",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "After evaluating the six proposed models, it has been found that incorporating a classification model significantly enhances performance in terms of precision (measured by the Dice coefficient) and speed. The combination of ResNetClass2 and SegFormer1 appears to be the most effective workflow, delivering the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c9828-9017-4bd9-8f13-5b4e984eeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Variables to save\n",
    "variables_to_save = {\n",
    "    \"score_ave_per_class_1\": score_ave_per_class_1,\n",
    "    \"score_ave_per_class_2_1\": score_ave_per_class_2_1,\n",
    "    \"time_per_image_1\": time_per_image_1,\n",
    "    \"score_ave_per_class_2\": score_ave_per_class_2,\n",
    "    \"score_ave_per_class_2_2\": score_ave_per_class_2_2,\n",
    "    \"time_per_image_2\": time_per_image_2,\n",
    "    \"score_ave_per_class_11\": score_ave_per_class_11,\n",
    "    \"score_ave_per_class_2_11\": score_ave_per_class_2_11,\n",
    "    \"time_per_image_11\": time_per_image_11,\n",
    "    \"score_ave_per_class_21\": score_ave_per_class_21,\n",
    "    \"score_ave_per_class_2_21\": score_ave_per_class_2_21,\n",
    "    \"time_per_image_21\": time_per_image_21,\n",
    "    \"score_ave_per_class_22\": score_ave_per_class_22,\n",
    "    \"score_ave_per_class_2_22\": score_ave_per_class_2_22,\n",
    "    \"time_per_image_22\": time_per_image_22,\n",
    "}\n",
    "\n",
    "# Specify the file path\n",
    "file_path = 'models/model_evaluation_variables.pkl'\n",
    "\n",
    "# Save variables to a file\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(variables_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6ebc6-ab8c-494b-9091-073bab4f12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path\n",
    "\n",
    "#file_path = 'models/model_evaluation_variables.pkl'\n",
    "\n",
    "# Load variables from the file\n",
    "#with open(file_path, 'rb') as f:\n",
    "#    loaded_variables = pickle.load(f)\n",
    "\n",
    "# Access the loaded variables\n",
    "#score_ave_per_class_1 = loaded_variables[\"score_ave_per_class_1\"]\n",
    "#score_ave_per_class_2_1 = loaded_variables[\"score_ave_per_class_2_1\"]\n",
    "#time_per_image_1 = loaded_variables[\"time_per_image_1\"]\n",
    "#score_ave_per_class_2 = loaded_variables[\"score_ave_per_class_2\"]\n",
    "#score_ave_per_class_2_2 = loaded_variables[\"score_ave_per_class_2_2\"]\n",
    "#time_per_image_2 = loaded_variables[\"time_per_image_2\"]\n",
    "#score_ave_per_class_11 = loaded_variables[\"score_ave_per_class_11\"]\n",
    "#score_ave_per_class_2_11 = loaded_variables[\"score_ave_per_class_2_11\"]\n",
    "#time_per_image_11 = loaded_variables[\"time_per_image_11\"]\n",
    "#score_ave_per_class_21 = loaded_variables[\"score_ave_per_class_21\"]\n",
    "#score_ave_per_class_2_21 = loaded_variables[\"score_ave_per_class_2_21\"]\n",
    "#time_per_image_21 = loaded_variables[\"time_per_image_21\"]\n",
    "#score_ave_per_class_22 = loaded_variables[\"score_ave_per_class_22\"]\n",
    "#score_ave_per_class_2_22 = loaded_variables[\"score_ave_per_class_2_22\"]\n",
    "#time_per_image_22 = loaded_variables[\"time_per_image_22\"]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_common",
   "language": "python",
   "name": ".venv_common"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299.896px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1378ba22f96e4134a3aadf23f395c097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50cd2c575f6345f5a159bfc600484d0e",
       "IPY_MODEL_86f3315ff10144f095aea40c461d256e",
       "IPY_MODEL_1700c995be8b45e2ae7676ae73dc88c5"
      ],
      "layout": "IPY_MODEL_3552cf8af21745129c9750e6ca619f29"
     }
    },
    "1700c995be8b45e2ae7676ae73dc88c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84878029a4fe45798f0c4728b738652d",
      "placeholder": "​",
      "style": "IPY_MODEL_4467c1d174224764bf4c0d4a8abe8dca",
      "value": " 2/2 [00:00&lt;00:00,  4.49it/s]"
     }
    },
    "17038ea0beb5498c8ef4b792cc4f78f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "178ab317e5ae44ad8640feb4c4db4837": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "23d8be2f4b884a0c9544b546a292f9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_714473c7d20848b6a742abfea94f3c52",
      "max": 280,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec5bbe8822bd4b34a784726a7671bca6",
      "value": 280
     }
    },
    "24fdc15d41624e5682b2e8322ea3a3f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a54a04435044109a737ba66ade2aafc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3552cf8af21745129c9750e6ca619f29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "4467c1d174224764bf4c0d4a8abe8dca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49140305690348abb2a551d3bdd3849e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24fdc15d41624e5682b2e8322ea3a3f0",
      "placeholder": "​",
      "style": "IPY_MODEL_738e6f0aea304c50a039395809b48216",
      "value": "Epoch 0: 100%"
     }
    },
    "50cd2c575f6345f5a159bfc600484d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a54a04435044109a737ba66ade2aafc",
      "placeholder": "​",
      "style": "IPY_MODEL_c195462a99934f639c166547547f433c",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "6647710a025145c09dc750f2dbf889bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f3f92ccc6324c1bbc1e4458b54ba521",
       "IPY_MODEL_23d8be2f4b884a0c9544b546a292f9bc",
       "IPY_MODEL_85d41b54e32d48a789c88379eb74d329"
      ],
      "layout": "IPY_MODEL_6c1a05d1bcb94ab5993b861b1a9b9c00"
     }
    },
    "6c1a05d1bcb94ab5993b861b1a9b9c00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "714473c7d20848b6a742abfea94f3c52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "738e6f0aea304c50a039395809b48216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76c71109197449199fe6b5126cbfdfcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8297e891e764434cb1c8007e6adb9fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84878029a4fe45798f0c4728b738652d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85d41b54e32d48a789c88379eb74d329": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe66361110e5430fb8e721025f6c7e75",
      "placeholder": "​",
      "style": "IPY_MODEL_76c71109197449199fe6b5126cbfdfcd",
      "value": " 280/280 [01:37&lt;00:00,  2.87it/s]"
     }
    },
    "86f3315ff10144f095aea40c461d256e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e654fa4cd4314f7a9f070558a7119659",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9a2e9b44f294265a9c7cacf4678d999",
      "value": 2
     }
    },
    "9f3f92ccc6324c1bbc1e4458b54ba521": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c306eacaeb3b47019480caf4a0467a03",
      "placeholder": "​",
      "style": "IPY_MODEL_d134b87624894ab4946ec1c07f0fa48d",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "be22a7e68186419f8bb2b1f6ff525402": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c195462a99934f639c166547547f433c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c306eacaeb3b47019480caf4a0467a03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9a2e9b44f294265a9c7cacf4678d999": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d134b87624894ab4946ec1c07f0fa48d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1d57cb119c644148b5ccaeb79dd60cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17038ea0beb5498c8ef4b792cc4f78f0",
      "placeholder": "​",
      "style": "IPY_MODEL_f3d9346390a4463c8d916d58101e290d",
      "value": " 1103/1103 [19:22&lt;00:00,  0.95it/s, v_num=zws9, train/batch_loss=nan.0, train/batch_f1=nan.0, valid/loss=nan.0, valid/f1=0.245]"
     }
    },
    "e59fc8cc7f784fceaf205d97685d263d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_49140305690348abb2a551d3bdd3849e",
       "IPY_MODEL_f05896a64ca640d085018265a319cec3",
       "IPY_MODEL_e1d57cb119c644148b5ccaeb79dd60cf"
      ],
      "layout": "IPY_MODEL_178ab317e5ae44ad8640feb4c4db4837"
     }
    },
    "e654fa4cd4314f7a9f070558a7119659": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec5bbe8822bd4b34a784726a7671bca6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f05896a64ca640d085018265a319cec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be22a7e68186419f8bb2b1f6ff525402",
      "max": 1103,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8297e891e764434cb1c8007e6adb9fa4",
      "value": 1103
     }
    },
    "f3d9346390a4463c8d916d58101e290d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe66361110e5430fb8e721025f6c7e75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
